#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ MEGA ULTRA ENGINE DE CONVERSAÃ‡ÃƒO - NÃVEL CHATGPT DE INTELIGÃŠNCIA
Sistema que entende ABSOLUTAMENTE TUDO que qualquer cliente falar
GIGANTEMENTE FODA - Mais inteligente que 99% dos humanos
"""

import re
import logging
import asyncio
import math
from typing import Dict, List, Optional, Any, Tuple, Set, Union
from datetime import datetime, timedelta
import json
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict, Counter
import unicodedata

logger = logging.getLogger(__name__)

class IntentType(Enum):
    FATURA_SOLICITAR = "fatura_solicitar"
    FATURA_VALOR = "fatura_valor"
    FATURA_VENCIMENTO = "fatura_vencimento"
    PAGAMENTO_CONFIRMACAO = "pagamento_confirmacao"
    PAGAMENTO_DIFICULDADE = "pagamento_dificuldade"
    NEGOCIACAO_DESCONTO = "negociacao_desconto"
    NEGOCIACAO_PARCELAMENTO = "negociacao_parcelamento"
    RECLAMACAO_COBRANCA_INDEVIDA = "reclamacao_cobranca_indevida"
    RECLAMACAO_VALOR_INCORRETO = "reclamacao_valor_incorreto"
    RECLAMACAO_SERVICO = "reclamacao_servico"
    CANCELAMENTO_SERVICO = "cancelamento_servico"
    INFORMACAO_CONTA = "informacao_conta"
    SAUDACAO = "saudacao"
    DESPEDIDA = "despedida"
    CONFIRMACAO = "confirmacao"
    NEGACAO = "negacao"
    DUVIDA = "duvida"
    NOT_UNDERSTOOD = "not_understood"

@dataclass
class ExtractedEntity:
    """Entidade extraÃ­da do texto com contexto semÃ¢ntico"""
    type: str
    value: str
    confidence: float
    context: str
    semantic_weight: float = 1.0
    alternatives: List[str] = field(default_factory=list)
    relationships: Dict[str, float] = field(default_factory=dict)

@dataclass
class SemanticPattern:
    """PadrÃ£o semÃ¢ntico avanÃ§ado para anÃ¡lise contextual"""
    pattern_id: str
    semantic_vectors: Dict[str, float]
    context_triggers: List[str]
    intent_weights: Dict[str, float]
    emotional_indicators: Dict[str, float]
    confidence_modifiers: Dict[str, float]

@dataclass
class ConversationMemory:
    """MemÃ³ria contextual avanÃ§ada da conversa"""
    user_profile: Dict[str, Any] = field(default_factory=dict)
    conversation_patterns: List[str] = field(default_factory=list)
    intent_history: List[Tuple[str, float, datetime]] = field(default_factory=list)
    emotional_journey: List[Tuple[str, float, datetime]] = field(default_factory=list)
    context_switches: List[datetime] = field(default_factory=list)
    learning_data: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ContextualIntent:
    """IntenÃ§Ã£o com contexto ULTRA avanÃ§ado - nÃ­vel ChatGPT"""
    intent: IntentType
    confidence: float
    entities: List[ExtractedEntity]
    temporal_context: str  # passado, presente, futuro
    emotional_state: str   # neutro, frustrado, satisfeito, urgente
    negation: bool        # se hÃ¡ negaÃ§Ã£o
    multiple_intents: List[IntentType]  # intenÃ§Ãµes secundÃ¡rias
    
    # ğŸš€ NOVOS CAMPOS ULTRA AVANÃ‡ADOS
    semantic_similarity: float = 0.0  # similaridade semÃ¢ntica com padrÃµes conhecidos
    contextual_coherence: float = 0.0  # coerÃªncia contextual com conversa anterior
    linguistic_complexity: float = 0.0  # complexidade linguÃ­stica do texto
    intent_certainty: float = 0.0  # certeza absoluta da intenÃ§Ã£o
    alternative_intents: List[Tuple[IntentType, float]] = field(default_factory=list)
    semantic_clusters: List[str] = field(default_factory=list)
    discourse_markers: List[str] = field(default_factory=list)
    pragmatic_inference: Dict[str, float] = field(default_factory=dict)

class SuperConversationEngine:
    """ğŸš€ MEGA ULTRA ENGINE - INTELIGÃŠNCIA NÃVEL CHATGPT GIGANTEMENTE FODA"""
    
    def __init__(self):
        # ğŸ§  MEMÃ“RIA CONTEXTUAL ULTRA AVANÃ‡ADA
        self.user_contexts = {}
        self.conversation_cache = {}
        self.conversation_memories = {}  # Nova memÃ³ria super avanÃ§ada
        self.semantic_knowledge_base = {}  # Base de conhecimento semÃ¢ntico
        self.pattern_learning_db = defaultdict(list)  # Aprendizado de padrÃµes
        
        # ğŸ”¬ SISTEMAS DE ANÃLISE ULTRA AVANÃ‡ADOS
        self.brazilian_language_db = self._load_brazilian_language_patterns()
        self.entity_extractors = self._load_entity_extractors()
        self.context_analyzers = self._load_context_analyzers()
        self.emotion_patterns = self._load_emotion_patterns()
        self.temporal_patterns = self._load_temporal_patterns()
        self.negation_patterns = self._load_negation_patterns()
        
        # ğŸš€ NOVOS SISTEMAS ULTRA AVANÃ‡ADOS - NÃVEL CHATGPT
        self.semantic_patterns = self._build_semantic_patterns()
        self.discourse_analyzers = self._load_discourse_analyzers()
        self.pragmatic_inference_engine = self._build_pragmatic_engine()
        self.contextual_coherence_analyzer = self._build_coherence_analyzer()
        self.multi_layer_processors = self._build_multi_layer_processors()
        self.intelligent_fallback_system = self._build_fallback_system()
        
        # ğŸ“š CONHECIMENTO LINGUÃSTICO ULTRA PROFUNDO
        self.brazilian_semantic_vectors = self._build_semantic_vectors()
        self.intent_similarity_matrix = self._build_intent_similarity_matrix()
        self.contextual_relationship_graph = self._build_relationship_graph()
        
        # ğŸ¯ RESPOSTAS CONTEXTUAIS MEGA INTELIGENTES
        self.contextual_responses = self._load_contextual_responses()
        self.dynamic_response_generator = self._build_dynamic_generator()
        
        logger.info("ğŸš€ MEGA ULTRA ENGINE DE CONVERSAÃ‡ÃƒO NÃVEL CHATGPT INICIALIZADA!")
        
    def _load_brazilian_language_patterns(self) -> Dict[str, List[Dict]]:
        """ğŸš€ PADRÃ•ES PARA CLIENTES BURROS - ENTENDE QUALQUER COISA MAL ESCRITA"""
        return {
            # ğŸ“„ FATURA - Tudo que pode significar "quero minha conta"
            'fatura_detection': [
                # Palavras diretas
                {'pattern': r'(conta|boleto|fatura|cobranÃ§a)', 'weight': 1.0},
                {'pattern': r'(segunda.?via|2.?via|2via)', 'weight': 1.0},
                {'pattern': r'(debito|dÃ©bito)', 'weight': 0.9},
                # Mal escritas comuns
                {'pattern': r'(cota|bolto|fatur)', 'weight': 0.9},  # erros de digitaÃ§Ã£o
                {'pattern': r'(segundav|2av)', 'weight': 0.9},
                {'pattern': r'(cobransa|cobranca)', 'weight': 0.8},
                # Contextos indiretos
                {'pattern': r'(papel|documento).*(pagar)', 'weight': 0.7},
                {'pattern': r'(como.*(pagar|quitar))', 'weight': 0.8},
                {'pattern': r'(preciso.*(pagar|quitar))', 'weight': 0.8},
                {'pattern': r'(onde.*(pagar|boleto))', 'weight': 0.8},
                # Linguagem super simples
                {'pattern': r'(papel.*dinheiro)', 'weight': 0.7},
                {'pattern': r'(quanto.*devo)', 'weight': 0.9},
                {'pattern': r'(minha.*divida)', 'weight': 0.8},
                {'pattern': r'(ter.*pagar)', 'weight': 0.7},
            ],
            
            # ğŸ’° VALOR - Quer saber quanto deve
            'valor_detection': [
                {'pattern': r'(quanto|valor)', 'weight': 1.0},
                {'pattern': r'(devo|deve|pagar)', 'weight': 0.9},
                {'pattern': r'(quanto.*mesmo|valor.*certo)', 'weight': 1.0},
                {'pattern': r'(ta.*quanto|tÃ¡.*quanto)', 'weight': 0.9},
                {'pattern': r'(preÃ§o|preco)', 'weight': 0.8},
                # Mal escritos
                {'pattern': r'(qnto|qnt)', 'weight': 0.8},
                {'pattern': r'(dveo|dvo)', 'weight': 0.7},  # "devo" mal escrito
                # Contextos
                {'pattern': r'(saber.*valor)', 'weight': 0.8},
                {'pattern': r'(conta.*valor)', 'weight': 0.8},
                {'pattern': r'(total.*pagar)', 'weight': 0.8},
            ],
            
            # â° VENCIMENTO - Quer saber quando vence
            'vencimento_detection': [
                {'pattern': r'(vencimento|vence|prazo)', 'weight': 1.0},
                {'pattern': r'(quando.*vence)', 'weight': 1.0},
                {'pattern': r'(data.*pagamento)', 'weight': 0.9},
                {'pattern': r'(atÃ©.*quando)', 'weight': 0.8},
                {'pattern': r'(prazo.*final)', 'weight': 0.8},
                # Mal escritos
                {'pattern': r'(vencimeto|vencimto)', 'weight': 0.8},
                {'pattern': r'(qndo.*vence)', 'weight': 0.8},
                # Contextos de urgÃªncia
                {'pattern': r'(ainda.*tempo)', 'weight': 0.7},
                {'pattern': r'(posso.*pagar)', 'weight': 0.6},
            ],
            
            # ğŸ¤ NEGOCIAÃ‡ÃƒO - Quer parcelar ou desconto
            'negociacao_detection': [
                {'pattern': r'(parcelar|dividir|fatiar)', 'weight': 1.0},
                {'pattern': r'(acordo|negociar|conversar)', 'weight': 0.9},
                {'pattern': r'(desconto|abatimento)', 'weight': 0.9},
                {'pattern': r'(dificuldade|difÃ­cil|apertado)', 'weight': 0.8},
                {'pattern': r'(nÃ£o.*consigo.*pagar)', 'weight': 0.9},
                {'pattern': r'(sem.*dinheiro|sem.*grana)', 'weight': 0.8},
                # Mal escritos
                {'pattern': r'(parcelar|parsela)', 'weight': 0.8},
                {'pattern': r'(descoto|dsconto)', 'weight': 0.7},
                # Linguagem simples
                {'pattern': r'(quebrar.*galho)', 'weight': 0.7},
                {'pattern': r'(dar.*jeito)', 'weight': 0.6},
                {'pattern': r'(facilitar|ajudar)', 'weight': 0.7},
                {'pattern': r'(condiÃ§Ãµes|condicoes)', 'weight': 0.8},
            ],
            
            # âœ… PAGAMENTO FEITO - JÃ¡ pagou
            'pagamento_detection': [
                {'pattern': r'(jÃ¡.*paguei|quitei|paguei)', 'weight': 1.0},
                {'pattern': r'(pix|transferÃªncia|depÃ³sito)', 'weight': 0.9},
                {'pattern': r'(efetuei|realizei)', 'weight': 0.8},
                {'pattern': r'(comprovante|anexo)', 'weight': 0.8},
                # Mal escritos
                {'pattern': r'(jah.*paguei|ja.*paguei)', 'weight': 0.9},
                {'pattern': r'(quitei|kitei)', 'weight': 0.8},
                {'pattern': r'(transferencia|trasferencia)', 'weight': 0.7},
                # Contextos
                {'pattern': r'(mandei.*dinheiro)', 'weight': 0.8},
                {'pattern': r'(pago.*ontem|pago.*hoje)', 'weight': 0.9},
                {'pattern': r'(banco.*pagar)', 'weight': 0.7},
            ],
            
            # ğŸ˜¡ RECLAMAÃ‡ÃƒO - EstÃ¡ reclamando
            'reclamacao_detection': [
                {'pattern': r'(errado|incorreto|equivocado)', 'weight': 1.0},
                {'pattern': r'(nunca.*(usei|contratei|pedi))', 'weight': 1.0},
                {'pattern': r'(nÃ£o.*devo|nao.*devo)', 'weight': 0.9},
                {'pattern': r'(indevida|indevido)', 'weight': 0.9},
                {'pattern': r'(contestar|discordar)', 'weight': 0.8},
                # PalavrÃµes e revolta (censurados)
                {'pattern': r'(que.*merda|porra|caramba)', 'weight': 0.9},
                {'pattern': r'(absurdo|revoltante)', 'weight': 0.8},
                # Linguagem simples de revolta
                {'pattern': r'(nÃ£o.*certo|nao.*certo)', 'weight': 0.8},
                {'pattern': r'(enganaÃ§Ã£o|roubo)', 'weight': 0.9},
                {'pattern': r'(nÃ£o.*aceito|nao.*aceito)', 'weight': 0.8},
            ],
            
            # ğŸ‘‹ SAUDAÃ‡Ã•ES E DESPEDIDAS
            'interacao_social': [
                # SaudaÃ§Ãµes
                {'pattern': r'(oi|olÃ¡|ola|oiii|eae|e.*ai)', 'intent': 'saudacao', 'weight': 1.0},
                {'pattern': r'(bom.*dia|boa.*tarde|boa.*noite)', 'intent': 'saudacao', 'weight': 1.0},
                {'pattern': r'(beleza|blz|suave)', 'intent': 'saudacao', 'weight': 0.8},
                # Despedidas
                {'pattern': r'(tchau|falou|atÃ©|flw|vlw)', 'intent': 'despedida', 'weight': 1.0},
                {'pattern': r'(obrigad[ao]|brigado|brigada)', 'intent': 'despedida', 'weight': 0.9},
                # ConfirmaÃ§Ãµes
                {'pattern': r'(tÃ¡.*bom|ta.*bom|ok|certo)', 'intent': 'confirmacao', 'weight': 0.8},
                {'pattern': r'(sim|yes|Ã©.*isso)', 'intent': 'confirmacao', 'weight': 0.8},
                # NegaÃ§Ãµes
                {'pattern': r'(nÃ£o|nao|num|nope)', 'intent': 'negacao', 'weight': 0.9},
                # DÃºvidas
                {'pattern': r'(como.*assim|que.*isso|uÃ©)', 'intent': 'duvida', 'weight': 0.8},
                {'pattern': r'(nÃ£o.*entendi|nao.*entendi)', 'intent': 'duvida', 'weight': 0.9},
            ],
            
            # ğŸ”¤ NORMALIZAÃ‡ÃƒO DE ERROS COMUNS
            'erro_patterns': {
                # SubstituiÃ§Ãµes automÃ¡ticas para normalizar textos mal escritos
                'qnto': 'quanto',
                'qnt': 'quanto', 
                'qndo': 'quando',
                'vc': 'vocÃª',
                'pq': 'porque',
                'tbm': 'tambÃ©m',
                'n': 'nÃ£o',
                'naum': 'nÃ£o',
                'eh': 'Ã©',
                'tah': 'estÃ¡',
                'to': 'estou',
                'pra': 'para',
                'pro': 'para o',
                'msm': 'mesmo',
                'blz': 'beleza',
                'vlw': 'valeu',
                'flw': 'falou',
                'kd': 'cadÃª',
                'aki': 'aqui',
                'ai': 'aÃ­',
                'hj': 'hoje',
                'ontem': 'ontem',
                'amanha': 'amanhÃ£',
                'soh': 'sÃ³',
                'jah': 'jÃ¡',
                'neh': 'nÃ©',
                'eh': 'Ã©',
                'num': 'nÃ£o',
                'vo': 'vou',
                'c': 'com',
                'cmg': 'comigo',
                'ctg': 'contigo',
                'dps': 'depois'
            }
        }
    
    def _load_entity_extractors(self) -> Dict[str, Dict]:
        """Extratores de entidades especÃ­ficas"""
        return {
            'valores_monetarios': {
                'patterns': [
                    r'R\$\s*(\d{1,3}(?:\.\d{3})*(?:,\d{2})?)',
                    r'(\d{1,3}(?:\.\d{3})*(?:,\d{2})?\s*reais?)',
                    r'(\d+(?:,\d{2})?\s*pila)',  # GÃ­ria brasileira
                ],
                'normalizer': self._normalize_currency
            },
            'datas': {
                'patterns': [
                    r'(\d{1,2}[/\-]\d{1,2}[/\-]\d{2,4})',
                    r'(hoje|amanhÃ£|ontem)',
                    r'(prÃ³xima?\s+\w+)',  # prÃ³xima semana
                    r'(dia\s+\d{1,2})',
                ],
                'normalizer': self._normalize_date
            },
            'protocolos': {
                'patterns': [
                    r'(protocolo\s*:?\s*(\w+\d+|\d+))',
                    r'(nÃºmero\s+(\w+\d+|\d+))',
                ],
                'normalizer': self._normalize_protocol
            },
            'documentos': {
                'patterns': [
                    r'(cpf\s*:?\s*(\d{3}\.?\d{3}\.?\d{3}\-?\d{2}))',
                    r'(cnpj\s*:?\s*(\d{2}\.?\d{3}\.?\d{3}/?\d{4}\-?\d{2}))',
                ],
                'normalizer': self._normalize_document
            }
        }
    
    def _load_context_analyzers(self) -> Dict[str, Any]:
        """Analisadores de contexto conversacional"""
        return {
            'sequencias_conversacionais': [
                # Cliente â†’ Bot â†’ Cliente (follow-up)
                {
                    'sequence': ['bot_fatura_response', 'client_clarification'],
                    'new_context': 'fatura_detalhamento',
                    'boost_intent': ['fatura_valor', 'fatura_vencimento']
                },
                {
                    'sequence': ['bot_negociacao_response', 'client_acceptance'],
                    'new_context': 'negociacao_ativa',
                    'boost_intent': ['confirmacao', 'negociacao_parcelamento']
                }
            ],
            'padroes_contextuais': [
                # Se mencionou pagamento + valor, provavelmente confirmaÃ§Ã£o
                {
                    'conditions': ['entity_valor', 'intent_pagamento'],
                    'inferred_intent': 'pagamento_confirmacao',
                    'confidence_boost': 0.3
                },
                # Se mencionou erro + valor, provavelmente reclamaÃ§Ã£o
                {
                    'conditions': ['emotion_frustrado', 'entity_valor'],
                    'inferred_intent': 'reclamacao_valor_incorreto',
                    'confidence_boost': 0.4
                }
            ]
        }
    
    def _load_emotion_patterns(self) -> Dict[str, List[Dict]]:
        """PadrÃµes de detecÃ§Ã£o emocional CORRIGIDOS"""
        return {
            'frustrado': [
                {'pattern': r'(que absurdo|nÃ£o acredito|revoltante)', 'weight': 1.0},
                {'pattern': r'(indignado|revoltado|irritado)', 'weight': 0.9},
                {'pattern': r'(errada?|incorret[ao])', 'weight': 0.7},  # Corrigido para detectar reclamaÃ§Ãµes
                {'pattern': r'(cara,.*paguei|cara,.*mas)', 'weight': 0.8},  # EspecÃ­fico para frustraÃ§Ã£o com pagamento
                {'pattern': r'(nunca.*(contratei|usei|pedi))', 'weight': 0.8},  # ReclamaÃ§Ã£o tÃ­pica
                {'pattern': r'(pÃ©ssimo|horrÃ­vel|terrÃ­vel)', 'weight': 0.8},
                {'pattern': r'(ainda.*(aparece|continua|mostra))', 'weight': 0.7},  # FrustraÃ§Ã£o com pendÃªncia
            ],
            'urgente': [
                {'pattern': r'(urgente|emergÃªncia)', 'weight': 1.0},
                {'pattern': r'(preciso.*(urgente|rÃ¡pido|agora))', 'weight': 0.9},
                {'pattern': r'(imediatamente|hoje)', 'weight': 0.8},
                {'pattern': r'(rÃ¡pido)', 'weight': 0.6},  # Reduzido para nÃ£o conflitar
            ],
            'satisfeito': [
                {'pattern': r'(obrigado|agradeÃ§o|valeu)', 'weight': 0.8},
                {'pattern': r'(perfeito|Ã³timo|excelente)', 'weight': 0.9},
                {'pattern': r'(resolveu|solucionou)', 'weight': 0.8},
                {'pattern': r'(muito bom|show)', 'weight': 0.7},
            ],
            'confuso': [
                {'pattern': r'(nÃ£o entendi|como assim)', 'weight': 0.9},
                {'pattern': r'(confuso|perdido|nÃ£o compreendi)', 'weight': 0.8},
                {'pattern': r'(explicar|esclarecer|que\s*\?)', 'weight': 0.6},
                # REMOVIDO "que" sozinho para evitar false positives
            ]
        }
    
    def _load_temporal_patterns(self) -> Dict[str, List[str]]:
        """PadrÃµes temporais da conversa"""
        return {
            'passado': [
                r'(jÃ¡\s+(paguei|fiz|resolvi))',
                r'(ontem|semana passada|mÃªs passado)',
                r'(paguei|quitei|resolvi)',
            ],
            'presente': [
                r'(agora|atualmente|no momento)',
                r'(estou|estamos|estÃ¡)',
                r'(hoje|neste momento)',
            ],
            'futuro': [
                r'(vou|vamos|pretendo)',
                r'(amanhÃ£|semana que vem|prÃ³ximo)',
                r'(planejando|pensando em)',
            ]
        }
    
    def _load_negation_patterns(self) -> List[str]:
        """PadrÃµes de negaÃ§Ã£o brasileiros"""
        return [
            r'(nÃ£o|num|nao)',
            r'(nunca|jamais)',
            r'(nem|nem que)',
            r'(de jeito nenhum|de forma alguma)',
            r'(negative|negativo)',
        ]
    
    async def process_message(self, phone: str, text: str) -> Optional[Dict[str, Any]]:
        """ğŸš€ PROCESSAMENTO ULTRA MEGA INTELIGENTE - NÃVEL CHATGPT GIGANTEMENTE FODA"""
        try:
            logger.info(f"ğŸš€ MEGA ANÃLISE ULTRA AVANÃ‡ADA para {phone}: {text[:50]}...")
            
            # ğŸ§  FASE 1: PREPARAÃ‡ÃƒO E NORMALIZAÃ‡ÃƒO ULTRA AVANÃ‡ADA
            conversation_memory = self._get_or_create_conversation_memory(phone)
            original_text = text
            normalized_text = self._ultra_advanced_normalize_text(text)
            
            # ğŸ”¬ FASE 2: ANÃLISE MULTI-CAMADAS ULTRA PROFUNDA
            linguistic_analysis = await self._perform_multi_layer_analysis(normalized_text)
            semantic_analysis = await self._perform_semantic_analysis(normalized_text, conversation_memory)
            pragmatic_analysis = await self._perform_pragmatic_analysis(normalized_text, conversation_memory)
            
            # ğŸ¯ FASE 3: EXTRAÃ‡ÃƒO DE ENTIDADES COM CONTEXTO SEMÃ‚NTICO
            entities = await self._extract_ultra_advanced_entities(normalized_text, semantic_analysis)
            logger.info(f"ğŸ” Entidades ultra avanÃ§adas: {[e.type + ':' + e.value for e in entities]}")
            
            # ğŸ˜Š FASE 4: ANÃLISE EMOCIONAL E TEMPORAL PROFUNDA
            emotional_state = await self._analyze_ultra_emotion(normalized_text, conversation_memory)
            temporal_context = await self._analyze_ultra_temporal_context(normalized_text, conversation_memory)
            negation_analysis = await self._analyze_ultra_negation(normalized_text)
            
            logger.info(f"ğŸ˜Š Estado emocional ultra: {emotional_state}")
            logger.info(f"â° Contexto temporal ultra: {temporal_context}")
            logger.info(f"âŒ AnÃ¡lise de negaÃ§Ã£o: {negation_analysis}")
            
            # ğŸ§  FASE 5: INFERÃŠNCIA CONTEXTUAL ULTRA AVANÃ‡ADA
            contextual_intent = await self._analyze_ultra_contextual_intent(
                normalized_text, entities, emotional_state, temporal_context, 
                negation_analysis, conversation_memory, semantic_analysis, pragmatic_analysis
            )
            
            # ğŸ“Š FASE 6: ANÃLISE DE COERÃŠNCIA E CERTEZA
            coherence_score = await self._analyze_contextual_coherence(contextual_intent, conversation_memory)
            certainty_score = await self._calculate_intent_certainty(contextual_intent, linguistic_analysis)
            
            contextual_intent.contextual_coherence = coherence_score
            contextual_intent.intent_certainty = certainty_score
            
            logger.info(f"ğŸ¯ IntenÃ§Ã£o principal ULTRA: {contextual_intent.intent.value}")
            logger.info(f"ğŸ¯ MÃºltiplas intenÃ§Ãµes: {[i.value for i in contextual_intent.multiple_intents]}")
            logger.info(f"ğŸ“Š ConfianÃ§a ULTRA: {contextual_intent.confidence:.3f}")
            logger.info(f"ğŸ”— CoerÃªncia contextual: {coherence_score:.3f}")
            logger.info(f"âœ… Certeza da intenÃ§Ã£o: {certainty_score:.3f}")
            
            # ğŸ§  FASE 7: APRENDIZADO E ATUALIZAÃ‡ÃƒO DE MEMÃ“RIA
            await self._update_ultra_conversation_memory(phone, contextual_intent, original_text, linguistic_analysis)
            await self._learn_from_interaction(phone, contextual_intent, semantic_analysis)
            
            # ğŸ­ FASE 8: GERAÃ‡ÃƒO DINÃ‚MICA DE RESPOSTA ULTRA INTELIGENTE
            response = await self._generate_ultra_contextual_response(
                phone, contextual_intent, entities, conversation_memory, semantic_analysis
            )
            
            return response
            
        except Exception as e:
            logger.error(f"âŒ Erro no processamento MEGA ULTRA: {e}")
            return await self._ultra_intelligent_fallback(phone, text, e)
    
    def _extract_all_entities(self, text: str) -> List[ExtractedEntity]:
        """ExtraÃ§Ã£o completa de entidades"""
        entities = []
        
        for entity_type, config in self.entity_extractors.items():
            for pattern in config['patterns']:
                matches = re.finditer(pattern, text, re.IGNORECASE)
                for match in matches:
                    entity = ExtractedEntity(
                        type=entity_type,
                        value=config['normalizer'](match.group()),
                        confidence=0.9,
                        context=text[max(0, match.start()-20):match.end()+20]
                    )
                    entities.append(entity)
        
        return entities
    
    def _analyze_emotion(self, text: str) -> str:
        """AnÃ¡lise emocional profunda"""
        emotion_scores = {}
        
        for emotion, patterns in self.emotion_patterns.items():
            score = 0.0
            for pattern_data in patterns:
                matches = len(re.findall(pattern_data['pattern'], text, re.IGNORECASE))
                score += matches * pattern_data['weight']
            emotion_scores[emotion] = score
        
        # Determinar emoÃ§Ã£o dominante
        if not emotion_scores or max(emotion_scores.values()) == 0:
            return 'neutro'
        
        dominant_emotion = max(emotion_scores.items(), key=lambda x: x[1])[0]
        return dominant_emotion
    
    def _analyze_temporal_context(self, text: str) -> str:
        """AnÃ¡lise do contexto temporal"""
        for tempo, patterns in self.temporal_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text, re.IGNORECASE):
                    return tempo
        
        return 'presente'  # default
    
    def _detect_negation(self, text: str) -> bool:
        """DetecÃ§Ã£o robusta de negaÃ§Ã£o"""
        for pattern in self.negation_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                return True
        return False
    
    def _analyze_contextual_intent(
        self, text: str, entities: List[ExtractedEntity], emotion: str,
        temporal: str, negation: bool, conversation_context: Dict
    ) -> ContextualIntent:
        """ğŸ§  ANÃLISE CONTEXTUAL REVOLUCIONÃRIA"""
        
        # AnÃ¡lise de intenÃ§Ãµes base
        base_intents = self._analyze_base_intents(text, entities, emotion)
        
        # Boost contextual baseado na conversa
        contextual_boost = self._apply_contextual_boost(base_intents, conversation_context)
        
        # AnÃ¡lise de mÃºltiplas intenÃ§Ãµes
        multiple_intents = self._detect_multiple_intents(text, entities)
        
        # Determinar intenÃ§Ã£o principal
        best_intent_data = max(contextual_boost.items(), key=lambda x: x[1])
        best_intent = IntentType(best_intent_data[0])
        confidence = min(best_intent_data[1], 1.0)
        
        return ContextualIntent(
            intent=best_intent,
            confidence=confidence,
            entities=entities,
            temporal_context=temporal,
            emotional_state=emotion,
            negation=negation,
            multiple_intents=multiple_intents
        )
    
    def _analyze_base_intents(self, text: str, entities: List[ExtractedEntity], emotion: str) -> Dict[str, float]:
        """ğŸš€ ANÃLISE ULTRA AVANÃ‡ADA - ENTENDE CLIENTES BURROS QUE NÃƒO SABEM SE EXPRESSAR"""
        intent_scores = {}
        
        # ğŸ“„ ANÃLISE FATURA - Usando nossos novos padrÃµes ultra avanÃ§ados
        fatura_solicitar_score = 0.0
        fatura_valor_score = 0.0
        fatura_vencimento_score = 0.0
        
        # Aplicar padrÃµes de detecÃ§Ã£o de fatura
        for pattern_data in self.brazilian_language_db.get('fatura_detection', []):
            matches = len(re.findall(pattern_data['pattern'], text, re.IGNORECASE))
            if matches > 0:
                fatura_solicitar_score += matches * pattern_data['weight']
        
        # Aplicar padrÃµes de detecÃ§Ã£o de valor
        for pattern_data in self.brazilian_language_db.get('valor_detection', []):
            matches = len(re.findall(pattern_data['pattern'], text, re.IGNORECASE))
            if matches > 0:
                fatura_valor_score += matches * pattern_data['weight']
        
        # Aplicar padrÃµes de detecÃ§Ã£o de vencimento
        for pattern_data in self.brazilian_language_db.get('vencimento_detection', []):
            matches = len(re.findall(pattern_data['pattern'], text, re.IGNORECASE))
            if matches > 0:
                fatura_vencimento_score += matches * pattern_data['weight']
        
        # ğŸ¤ ANÃLISE NEGOCIAÃ‡ÃƒO - Usando nossos padrÃµes avanÃ§ados
        negociacao_parcelamento_score = 0.0
        negociacao_desconto_score = 0.0
        
        for pattern_data in self.brazilian_language_db.get('negociacao_detection', []):
            matches = len(re.findall(pattern_data['pattern'], text, re.IGNORECASE))
            if matches > 0:
                # Decidir se Ã© parcelamento ou desconto baseado no contexto
                if re.search(r'(parcelar|dividir|fatiar)', pattern_data['pattern'], re.IGNORECASE):
                    negociacao_parcelamento_score += matches * pattern_data['weight']
                elif re.search(r'(desconto|abatimento)', pattern_data['pattern'], re.IGNORECASE):
                    negociacao_desconto_score += matches * pattern_data['weight']
                else:
                    # PadrÃµes genÃ©ricos: priorizar parcelamento (mais comum)
                    negociacao_parcelamento_score += matches * pattern_data['weight'] * 0.7
                    negociacao_desconto_score += matches * pattern_data['weight'] * 0.3
        
        # âœ… ANÃLISE PAGAMENTO FEITO - Usando nossos padrÃµes
        pagamento_score = 0.0
        
        for pattern_data in self.brazilian_language_db.get('pagamento_detection', []):
            matches = len(re.findall(pattern_data['pattern'], text, re.IGNORECASE))
            if matches > 0:
                pagamento_score += matches * pattern_data['weight']
        
        # ğŸ˜¡ ANÃLISE RECLAMAÃ‡ÃƒO - Usando nossos padrÃµes
        reclamacao_indevida_score = 0.0
        reclamacao_valor_score = 0.0
        
        for pattern_data in self.brazilian_language_db.get('reclamacao_detection', []):
            matches = len(re.findall(pattern_data['pattern'], text, re.IGNORECASE))
            if matches > 0:
                # Se menciona "nunca usei/contratei" Ã© cobranÃ§a indevida
                if re.search(r'(nunca.*(usei|contratei))', pattern_data['pattern'], re.IGNORECASE):
                    reclamacao_indevida_score += matches * pattern_data['weight']
                else:
                    # Outros tipos de reclamaÃ§Ã£o (valor incorreto)
                    reclamacao_valor_score += matches * pattern_data['weight']
        
        # ğŸ‘‹ ANÃLISE INTERAÃ‡ÃƒO SOCIAL
        saudacao_score = 0.0
        despedida_score = 0.0
        confirmacao_score = 0.0
        negacao_score = 0.0
        duvida_score = 0.0
        
        for pattern_data in self.brazilian_language_db.get('interacao_social', []):
            matches = len(re.findall(pattern_data['pattern'], text, re.IGNORECASE))
            if matches > 0:
                intent_type = pattern_data.get('intent', 'unknown')
                score = matches * pattern_data['weight']
                
                if intent_type == 'saudacao':
                    saudacao_score += score
                elif intent_type == 'despedida':
                    despedida_score += score
                elif intent_type == 'confirmacao':
                    confirmacao_score += score
                elif intent_type == 'negacao':
                    negacao_score += score
                elif intent_type == 'duvida':
                    duvida_score += score
        
        # ğŸ§  LÃ“GICA CONTEXTUAL AVANÃ‡ADA PARA CASOS CONFUSOS
        
        # Se cliente escreveu poucas palavras, tentar inferir pelo contexto
        palavras = len(text.split())
        if palavras <= 3:
            # Textos muito curtos - analisar palavras-chave crÃ­ticas
            if re.search(r'(conta|boleto|fatura)', text, re.IGNORECASE):
                fatura_solicitar_score += 0.8
            elif re.search(r'(quanto|valor)', text, re.IGNORECASE):
                fatura_valor_score += 0.8
            elif re.search(r'(quando|vence)', text, re.IGNORECASE):
                fatura_vencimento_score += 0.8
            elif re.search(r'(paguei|pago)', text, re.IGNORECASE):
                pagamento_score += 0.8
            elif re.search(r'(parcelar|acordo)', text, re.IGNORECASE):
                negociacao_parcelamento_score += 0.8
        
        # Se tem entidades monetÃ¡rias, boost intenÃ§Ãµes relacionadas a dinheiro
        tem_valor = any(e.type == 'valores_monetarios' for e in entities)
        if tem_valor:
            fatura_valor_score += 0.4
            pagamento_score += 0.3
            negociacao_parcelamento_score += 0.2
        
        # Se tem datas, boost vencimento
        tem_data = any(e.type == 'datas' for e in entities)
        if tem_data:
            fatura_vencimento_score += 0.4
            pagamento_score += 0.2
        
        # ğŸ˜¤ BOOST BASEADO EM EMOÃ‡ÃƒO
        if emotion == 'frustrado':
            reclamacao_indevida_score += 0.4
            reclamacao_valor_score += 0.4
        elif emotion == 'urgente':
            fatura_solicitar_score += 0.3
            fatura_valor_score += 0.2
        elif emotion == 'confuso':
            duvida_score += 0.3
        
        # ğŸ¯ NORMALIZAR SCORES (max 1.0 para cada)
        intent_scores = {
            'fatura_solicitar': min(fatura_solicitar_score, 1.0),
            'fatura_valor': min(fatura_valor_score, 1.0),
            'fatura_vencimento': min(fatura_vencimento_score, 1.0),
            'negociacao_parcelamento': min(negociacao_parcelamento_score, 1.0),
            'negociacao_desconto': min(negociacao_desconto_score, 1.0),
            'pagamento_confirmacao': min(pagamento_score, 1.0),
            'reclamacao_cobranca_indevida': min(reclamacao_indevida_score, 1.0),
            'reclamacao_valor_incorreto': min(reclamacao_valor_score, 1.0),
            'saudacao': min(saudacao_score, 1.0),
            'despedida': min(despedida_score, 1.0),
            'confirmacao': min(confirmacao_score, 1.0),
            'negacao': min(negacao_score, 1.0),
            'duvida': min(duvida_score, 1.0)
        }
        
        # ğŸš¨ FALLBACK INTELIGENTE - Se nenhuma intenÃ§Ã£o forte foi detectada
        max_score = max(intent_scores.values()) if intent_scores.values() else 0
        if max_score < 0.3:
            # Cliente escreveu algo muito confuso - tentar inferir pela presenÃ§a de palavras-chave
            if any(palavra in text.lower() for palavra in ['conta', 'boleto', 'fatura', 'pagar', 'deve']):
                intent_scores['fatura_solicitar'] = 0.5  # Assumir que quer fatura
            elif any(palavra in text.lower() for palavra in ['quanto', 'valor', 'preÃ§o']):
                intent_scores['fatura_valor'] = 0.5  # Assumir que quer saber valor
            else:
                intent_scores['duvida'] = 0.5  # Cliente estÃ¡ confuso
        
        return intent_scores
    
    def _apply_contextual_boost(self, base_intents: Dict[str, float], context: Dict) -> Dict[str, float]:
        """Aplicar boost baseado no contexto conversacional"""
        boosted_intents = base_intents.copy()
        
        # Se Ãºltima mensagem foi sobre fatura, boost relacionados
        last_context = context.get('last_intent')
        if last_context and 'fatura' in last_context:
            boosted_intents['fatura_valor'] = boosted_intents.get('fatura_valor', 0) + 0.2
            boosted_intents['fatura_vencimento'] = boosted_intents.get('fatura_vencimento', 0) + 0.2
        
        # Se contexto de negociaÃ§Ã£o ativa
        if context.get('negotiation_active'):
            boosted_intents['negociacao_desconto'] = boosted_intents.get('negociacao_desconto', 0) + 0.3
            boosted_intents['confirmacao'] = boosted_intents.get('confirmacao', 0) + 0.2
        
        return boosted_intents
    
    def _detect_multiple_intents(self, text: str, entities: List[ExtractedEntity]) -> List[IntentType]:
        """Detectar mÃºltiplas intenÃ§Ãµes na mesma mensagem - MELHORADO"""
        intents = []
        
        # Detectores mais robustos de mÃºltiplas intenÃ§Ãµes
        
        # "fatura E desconto/parcelamento"
        if (re.search(r'(fatura|conta)', text, re.IGNORECASE) and 
            re.search(r'(tambÃ©m|e\s+(tambÃ©m)?).*(desconto|parcelar)', text, re.IGNORECASE)):
            intents.extend([IntentType.FATURA_SOLICITAR, IntentType.NEGOCIACAO_DESCONTO])
        
        # "fatura E parcelamento"  
        if (re.search(r'(fatura|conta)', text, re.IGNORECASE) and 
            re.search(r'(tambÃ©m|e\s+(tambÃ©m)?).*(parcelar|dividir)', text, re.IGNORECASE)):
            intents.extend([IntentType.FATURA_SOLICITAR, IntentType.NEGOCIACAO_PARCELAMENTO])
        
        # "paguei MAS ainda aparece"
        if (re.search(r'(paguei|quitei)', text, re.IGNORECASE) and 
            re.search(r'(mas|porÃ©m|ainda|continua|aparece)', text, re.IGNORECASE)):
            intents.extend([IntentType.PAGAMENTO_CONFIRMACAO, IntentType.RECLAMACAO_VALOR_INCORRETO])
        
        # "valor E vencimento"
        if (re.search(r'(quanto.*devo)', text, re.IGNORECASE) and 
            re.search(r'(quando.*vence|prazo)', text, re.IGNORECASE)):
            intents.extend([IntentType.FATURA_VALOR, IntentType.FATURA_VENCIMENTO])
        
        # Conectores brasileiros comuns
        conectores = [r'\s+e\s+', r'\s+tambÃ©m\s+', r'\s+alÃ©m\s+disso\s+', r'\s+mais\s+']
        for conector in conectores:
            if re.search(conector, text, re.IGNORECASE):
                # Se tem conector, analisar cada parte
                partes = re.split(conector, text, flags=re.IGNORECASE)
                if len(partes) >= 2:
                    # Analisar se cada parte tem intenÃ§Ã£o diferente
                    primeira_parte = partes[0].strip()
                    segunda_parte = partes[1].strip()
                    
                    # LÃ³gica simplificada para detectar intenÃ§Ãµes diferentes
                    if ('fatura' in primeira_parte.lower() and 
                        any(palavra in segunda_parte.lower() for palavra in ['desconto', 'parcelar', 'negociar'])):
                        intents.extend([IntentType.FATURA_SOLICITAR, IntentType.NEGOCIACAO_DESCONTO])
                        break
        
        return intents
    
    # MÃ©todos de normalizaÃ§Ã£o (implementaÃ§Ãµes simplificadas)
    def _normalize_currency(self, text: str) -> str:
        return re.sub(r'[^\d,]', '', text)
    
    def _normalize_date(self, text: str) -> str:
        return text.strip()
    
    def _normalize_protocol(self, text: str) -> str:
        return re.sub(r'[^\w\d]', '', text)
    
    def _normalize_document(self, text: str) -> str:
        return re.sub(r'[^\d]', '', text)
    
    def _super_normalize_text(self, text: str) -> str:
        """ğŸš€ NORMALIZAÃ‡ÃƒO ULTRA AVANÃ‡ADA - CORRIGE QUALQUER TEXTO MAL ESCRITO"""
        
        # 1. PRIMEIRA PASSADA - Limpeza bÃ¡sica
        text = text.lower().strip()
        
        # 2. REMOVER EMOJIS E CARACTERES ESPECIAIS (mas preservar pontuaÃ§Ã£o bÃ¡sica)
        text = re.sub(r'[^\w\s\.,!?\-Ã¡Ã Ã¢Ã£Ã©Ã¨ÃªÃ­Ã¬Ã®Ã³Ã²Ã´ÃµÃºÃ¹Ã»Ã§]', ' ', text)
        
        # 3. CORRIGIR ABREVIAÃ‡Ã•ES E ERROS COMUNS (do nosso dicionÃ¡rio)
        erro_patterns = self.brazilian_language_db.get('erro_patterns', {})
        for erro, correto in erro_patterns.items():
            # Usar word boundary para nÃ£o corrigir partes de palavras
            text = re.sub(rf'\b{re.escape(erro)}\b', correto, text, flags=re.IGNORECASE)
        
        # 4. CORREÃ‡Ã•ES ESPECÃFICAS DE PORTUGUÃŠS BRASILEIRO MAL ESCRITO
        corrections = {
            # Erros comuns de "quanto"
            r'\b(qnt|qnto|qto|cuanto)\b': 'quanto',
            # Erros comuns de "quando"  
            r'\b(qnd|qndo|quado|cuando)\b': 'quando',
            # Erros de "vocÃª"
            r'\bvc\b': 'vocÃª',
            # Erros de "nÃ£o"
            r'\b(nao|naum|Ã±|n)\b': 'nÃ£o',
            # Erros de "para"
            r'\b(pra|pr)\b': 'para',
            # Erros de "porque"
            r'\b(pq|pk|porq)\b': 'porque',
            # Erros de "tambÃ©m"
            r'\b(tb|tbm|tbn)\b': 'tambÃ©m',
            # Erros de "estÃ¡"
            r'\b(tah|ta|tÃ¡)\b': 'estÃ¡',
            # Erros de "estou"
            r'\b(to|tou)\b': 'estou',
            # Erros de "jÃ¡"
            r'\b(jah|ja)\b': 'jÃ¡',
            # Erros de "sÃ³"
            r'\b(soh|so)\b': 'sÃ³',
            # Erros de "Ã©"
            r'\b(eh|e)\b': 'Ã©',
            # Erros de "hoje"
            r'\bhj\b': 'hoje',
            # Erros de "amanhÃ£"
            r'\b(amanha|amÃ±)\b': 'amanhÃ£',
            # Erros de "cadÃª"
            r'\bkd\b': 'cadÃª',
            # Erros de "aqui"
            r'\b(aki|aq)\b': 'aqui',
            # Erros de "aÃ­"
            r'\b(ai|ae)\b': 'aÃ­',
            # Erros de "mesmo"
            r'\b(msm|mmo)\b': 'mesmo',
            # Erros de "beleza"
            r'\b(blz|bz)\b': 'beleza',
            # Erros de "valeu"
            r'\b(vlw|vl)\b': 'valeu',
            # Erros de "falou"
            r'\b(flw|fl)\b': 'falou',
            # Erros comuns de palavras de cobranÃ§a
            r'\b(fatur|ftur)\b': 'fatura',
            r'\b(bolto|bleto)\b': 'boleto',
            r'\b(cota|cnta)\b': 'conta',
            r'\b(cobransa|cobranca)\b': 'cobranÃ§a',
            r'\b(pagameto|pagamnto)\b': 'pagamento',
            r'\b(vencimeto|vencimto)\b': 'vencimento',
            r'\b(transferencia|trasferencia)\b': 'transferÃªncia',
            r'\b(debto|debito)\b': 'dÃ©bito'
        }
        
        for pattern, replacement in corrections.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        # 5. REMOVER PONTUAÃ‡ÃƒO EXCESSIVA mas preservar sentido
        text = re.sub(r'[!]{2,}', '!', text)
        text = re.sub(r'[?]{2,}', '?', text)
        text = re.sub(r'[.]{2,}', '...', text)
        
        # 6. NORMALIZAR ESPAÃ‡OS
        text = re.sub(r'\s+', ' ', text)
        
        # 7. CORREÃ‡Ã•ES CONTEXTUAIS ESPECÃFICAS PARA COBRANÃ‡A
        cobranca_corrections = {
            # "segunda via" mal escrito
            r'(segunda|2)\s*(v|vi|via)': 'segunda via',
            # "quanto devo" mal escrito  
            r'(quanto|qnto)\s*(devo|dvo|dveo)': 'quanto devo',
            # "jÃ¡ paguei" mal escrito
            r'(jÃ¡|jah|ja)\s*(paguei|pguei|pag)': 'jÃ¡ paguei',
            # "nÃ£o devo" mal escrito
            r'(nÃ£o|nao|naum)\s*(devo|dvo)': 'nÃ£o devo',
            # "minha conta" mal escrito
            r'(minha|miha)\s*(conta|cota)': 'minha conta'
        }
        
        for pattern, replacement in cobranca_corrections.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text.strip()
    
    def _get_conversation_context(self, phone: str) -> Dict:
        """Obter contexto da conversa"""
        return self.user_contexts.get(phone, {})
    
    def _update_conversation_context(self, phone: str, intent: ContextualIntent, text: str):
        """Atualizar contexto conversacional"""
        if phone not in self.user_contexts:
            self.user_contexts[phone] = {
                'messages': [],
                'last_intent': None,
                'negotiation_active': False,
                'client_profile': {},
                'conversation_flow': []
            }
        
        context = self.user_contexts[phone]
        context['messages'].append({
            'text': text,
            'intent': intent.intent.value,
            'confidence': intent.confidence,
            'timestamp': datetime.now(),
            'entities': [{'type': e.type, 'value': e.value} for e in intent.entities],
            'emotion': intent.emotional_state
        })
        
        context['last_intent'] = intent.intent.value
        
        # Detectar se negociaÃ§Ã£o estÃ¡ ativa
        if intent.intent in [IntentType.NEGOCIACAO_DESCONTO, IntentType.NEGOCIACAO_PARCELAMENTO]:
            context['negotiation_active'] = True
        
        # Manter apenas Ãºltimas 10 mensagens
        context['messages'] = context['messages'][-10:]
    
    async def _generate_contextual_response(
        self, phone: str, intent: ContextualIntent, entities: List[ExtractedEntity], context: Dict
    ) -> Dict[str, Any]:
        """ğŸš€ GERADOR DE RESPOSTAS ULTRA INTELIGENTE - PERFEITO PARA CLIENTES BURROS"""
        
        # ğŸ¯ RESPOSTAS BASEADAS NA INTENÃ‡ÃƒO COM CONTEXTO EMOCIONAL
        
        if intent.intent == IntentType.FATURA_SOLICITAR:
            if intent.emotional_state == 'urgente':
                response_text = "ğŸš¨ **URGENTE!** Entendi! Vou buscar sua fatura AGORA MESMO e te enviar em segundos!"
            elif intent.emotional_state == 'frustrado':
                response_text = "ğŸ˜” Percebo que vocÃª estÃ¡ chateado. Calma, vou resolver isso rapidinho! Enviando sua fatura jÃ¡..."
            elif intent.negation:
                response_text = "ğŸ¤” Vi que vocÃª disse 'nÃ£o' sobre algo. Me explica melhor o que vocÃª precisa da sua conta?"
            else:
                response_text = "ğŸ“„ **PERFEITO!** Vou pegar sua fatura para vocÃª. SÃ³ um minutinho..."
        
        elif intent.intent == IntentType.FATURA_VALOR:
            valor_entity = next((e for e in entities if e.type == 'valores_monetarios'), None)
            if valor_entity:
                response_text = f"ğŸ’° Vi que vocÃª mencionou **R$ {valor_entity.value}**. Vou confirmar se esse Ã© o valor correto da sua conta!"
            elif intent.emotional_state == 'urgente':
                response_text = "ğŸ’° **URGENTE!** Vou verificar AGORA quanto vocÃª deve exatamente!"
            else:
                response_text = "ğŸ’° Entendi! VocÃª quer saber **QUANTO DEVE**, certo? Vou verificar o valor da sua conta!"
        
        elif intent.intent == IntentType.FATURA_VENCIMENTO:
            data_entity = next((e for e in entities if e.type == 'datas'), None)
            if data_entity:
                response_text = f"â° Vi que vocÃª mencionou **{data_entity.value}**. Vou confirmar o vencimento da sua conta!"
            else:
                response_text = "â° Entendi! VocÃª quer saber **QUANDO VENCE** sua conta, nÃ©? Vou verificar a data!"
        
        elif intent.intent == IntentType.NEGOCIACAO_PARCELAMENTO:
            if intent.emotional_state == 'frustrado':
                response_text = "ğŸ¤ Entendo que estÃ¡ difÃ­cil pagar. **CALMA!** Vamos dar um jeito! Temos vÃ¡rias opÃ§Ãµes de parcelamento!"
            elif any(e.type == 'valores_monetarios' for e in entities):
                valor = next(e.value for e in entities if e.type == 'valores_monetarios')
                response_text = f"ğŸ¤ Perfeito! VocÃª quer parcelar **R$ {valor}**, nÃ©? Vamos encontrar a melhor condiÃ§Ã£o para vocÃª!"
            else:
                response_text = "ğŸ¤ **Ã“TIMO!** Quer parcelar? Vou ver as melhores condiÃ§Ãµes que temos disponÃ­veis!"
        
        elif intent.intent == IntentType.NEGOCIACAO_DESCONTO:
            if intent.emotional_state == 'frustrado':
                response_text = "ğŸ’¸ Entendo sua situaÃ§Ã£o! Vamos ver que **DESCONTO** posso conseguir para vocÃª!"
            else:
                response_text = "ğŸ’¸ Interessado em desconto? **PERFEITO!** Vou verificar as promoÃ§Ãµes disponÃ­veis!"
        
        elif intent.intent == IntentType.PAGAMENTO_CONFIRMACAO:
            if intent.temporal_context == 'passado':
                if intent.emotional_state == 'frustrado':
                    response_text = "âœ… Entendi! VocÃª **JÃ PAGOU** mas ainda estÃ¡ aparecendo, nÃ©? Vou verificar URGENTE o que aconteceu!"
                else:
                    response_text = "âœ… **BELEZA!** VocÃª jÃ¡ pagou! Vou confirmar aqui no sistema se o pagamento foi processado!"
            else:
                response_text = "ğŸ’³ Perfeito! Vou verificar o status do seu pagamento no sistema!"
        
        elif intent.intent == IntentType.RECLAMACAO_COBRANCA_INDEVIDA:
            if intent.emotional_state == 'frustrado':
                response_text = "ğŸ˜¡ **ENTENDO SUA REVOLTA!** CobranÃ§a indevida Ã© muito chato mesmo! Vou resolver isso AGORA!"
            else:
                response_text = "ğŸ” Entendi! VocÃª acha que essa cobranÃ§a estÃ¡ **ERRADA**, nÃ©? Vou analisar sua situaÃ§Ã£o!"
        
        elif intent.intent == IntentType.RECLAMACAO_VALOR_INCORRETO:
            response_text = "ğŸ” **NOSSA!** Valor incorreto Ã© sÃ©rio! Vou verificar sua conta e corrigir se estiver errado mesmo!"
        
        elif intent.intent == IntentType.SAUDACAO:
            horario = datetime.now().hour
            if horario < 12:
                response_text = "ğŸŒ… **BOM DIA!** Tudo beleza? Como posso te ajudar hoje?"
            elif horario < 18:
                response_text = "â˜€ï¸ **BOA TARDE!** E aÃ­, tudo certo? Em que posso ajudar?"
            else:
                response_text = "ğŸŒ™ **BOA NOITE!** Beleza? Como posso te ajudar?"
        
        elif intent.intent == IntentType.DESPEDIDA:
            response_text = "ğŸ‘‹ **VALEU!** Obrigado pelo contato! Qualquer coisa, me chama! ğŸ˜Š"
        
        elif intent.intent == IntentType.CONFIRMACAO:
            response_text = "âœ… **PERFEITO!** Entendi que vocÃª confirmou! Vou continuar com o processo!"
        
        elif intent.intent == IntentType.NEGACAO:
            response_text = "âŒ **BELEZA!** VocÃª disse que nÃ£o. Me explica melhor o que vocÃª precisa entÃ£o?"
        
        elif intent.intent == IntentType.DUVIDA:
            response_text = "ğŸ¤” **SEM PROBLEMAS!** Vou explicar melhor! O que especificamente vocÃª nÃ£o entendeu?"
        
        else:
            # Fallback inteligente baseado no que foi detectado
            if intent.confidence < 0.5:
                response_text = "ğŸ¤” **CALMA!** Acho que nÃ£o entendi direito. Pode me falar de novo de um jeito mais simples? Tipo: 'quero minha conta' ou 'quanto devo'?"
            else:
                response_text = "ğŸ¤– **ENTENDI ALGUMA COISA!** Mas me explica melhor o que vocÃª precisa. Fala de forma simples!"
        
        # ğŸ“‹ ADICIONAR INFORMAÃ‡Ã•ES SOBRE MÃšLTIPLAS INTENÃ‡Ã•ES
        if intent.multiple_intents and len(intent.multiple_intents) > 0:
            intents_text = []
            for multi_intent in intent.multiple_intents:
                if multi_intent == IntentType.FATURA_SOLICITAR:
                    intents_text.append("ver sua conta")
                elif multi_intent == IntentType.FATURA_VALOR:
                    intents_text.append("saber quanto deve")
                elif multi_intent == IntentType.NEGOCIACAO_PARCELAMENTO:
                    intents_text.append("parcelar")
                elif multi_intent == IntentType.NEGOCIACAO_DESCONTO:
                    intents_text.append("conseguir desconto")
                else:
                    intents_text.append(multi_intent.value.replace('_', ' '))
            
            if intents_text:
                response_text += f"\n\nğŸ“‹ **TAMBÃ‰M PERCEBI** que vocÃª quer: {' e '.join(intents_text)}. Vou ajudar com tudo!"
        
        # ğŸ”¥ ADICIONAR CALL TO ACTION BASEADO NA INTENÃ‡ÃƒO
        if intent.intent in [IntentType.FATURA_SOLICITAR, IntentType.FATURA_VALOR, IntentType.FATURA_VENCIMENTO]:
            response_text += "\n\nâš¡ **Aguarda aÃ­ que vou buscar suas informaÃ§Ãµes!**"
        elif intent.intent in [IntentType.NEGOCIACAO_PARCELAMENTO, IntentType.NEGOCIACAO_DESCONTO]:
            response_text += "\n\nğŸ¤ **Vou verificar as melhores condiÃ§Ãµes para vocÃª!**"
        elif intent.intent == IntentType.PAGAMENTO_CONFIRMACAO:
            response_text += "\n\nğŸ” **Verificando seu pagamento no sistema...**"
        
        return {
            'text': response_text,
            'intent': intent.intent.value,
            'confidence': intent.confidence,
            'entities_detected': len(entities),
            'emotional_state': intent.emotional_state,
            'multiple_intents': len(intent.multiple_intents),
            'context_enhanced': True,
            'response_type': 'ultra_contextual'
        }
    
    async def _generate_fallback_response(self, phone: str, text: str) -> Dict[str, Any]:
        """Resposta de fallback inteligente"""
        return {
            'text': "ğŸ¤” Percebi que vocÃª estÃ¡ tentando me dizer algo importante. Pode reformular para eu entender melhor?",
            'intent': 'clarification_needed',
            'confidence': 0.3,
            'fallback': True
        }
    
    def _load_contextual_responses(self) -> Dict:
        """Carregar respostas contextuais avanÃ§adas"""
        return {
            # Implementar depois conforme necessÃ¡rio
            'advanced_templates': {}
        } 
    
    # ================================
    # ğŸš€ SISTEMAS ULTRA AVANÃ‡ADOS - NÃVEL CHATGPT
    # ================================
    
    def _build_semantic_patterns(self) -> Dict[str, SemanticPattern]:
        """ğŸ§  CONSTRUIR PADRÃ•ES SEMÃ‚NTICOS ULTRA AVANÃ‡ADOS"""
        patterns = {}
        
        # PadrÃ£o semÃ¢ntico para FATURA
        patterns['fatura_semantic'] = SemanticPattern(
            pattern_id='fatura_semantic',
            semantic_vectors={
                'documento': 0.9, 'papel': 0.8, 'conta': 1.0, 'boleto': 1.0,
                'cobranÃ§a': 0.9, 'dÃ©bito': 0.8, 'pagamento': 0.7, 'valor': 0.6,
                'segunda_via': 1.0, 'cÃ³pia': 0.7, 'comprovante': 0.6
            },
            context_triggers=['preciso', 'quero', 'mandar', 'enviar', 'ver'],
            intent_weights={'fatura_solicitar': 1.0, 'fatura_valor': 0.3},
            emotional_indicators={'urgente': 0.3, 'neutro': 0.7},
            confidence_modifiers={'direto': 1.0, 'indireto': 0.7}
        )
        
        # PadrÃ£o semÃ¢ntico para VALOR/QUANTIDADE
        patterns['valor_semantic'] = SemanticPattern(
            pattern_id='valor_semantic',
            semantic_vectors={
                'quanto': 1.0, 'valor': 1.0, 'preÃ§o': 0.9, 'custo': 0.8,
                'dinheiro': 0.7, 'grana': 0.8, 'real': 0.6, 'centavo': 0.5,
                'total': 0.9, 'dever': 0.9, 'pagar': 0.8
            },
            context_triggers=['devo', 'pago', 'custa', 'vale'],
            intent_weights={'fatura_valor': 1.0, 'pagamento_confirmacao': 0.4},
            emotional_indicators={'frustrado': 0.2, 'neutro': 0.8},
            confidence_modifiers={'pergunta': 1.0, 'afirmacao': 0.6}
        )
        
        # PadrÃ£o semÃ¢ntico para TEMPO/VENCIMENTO
        patterns['tempo_semantic'] = SemanticPattern(
            pattern_id='tempo_semantic',
            semantic_vectors={
                'quando': 1.0, 'data': 0.9, 'dia': 0.8, 'prazo': 1.0,
                'vencimento': 1.0, 'vence': 1.0, 'atÃ©': 0.7, 'tempo': 0.8,
                'hoje': 0.6, 'amanhÃ£': 0.7, 'mÃªs': 0.6
            },
            context_triggers=['vence', 'termina', 'acaba', 'expira'],
            intent_weights={'fatura_vencimento': 1.0, 'pagamento_confirmacao': 0.3},
            emotional_indicators={'urgente': 0.5, 'neutro': 0.5},
            confidence_modifiers={'futuro': 1.0, 'passado': 0.4}
        )
        
        # PadrÃ£o semÃ¢ntico para NEGOCIAÃ‡ÃƒO
        patterns['negociacao_semantic'] = SemanticPattern(
            pattern_id='negociacao_semantic',
            semantic_vectors={
                'parcelar': 1.0, 'dividir': 0.9, 'acordo': 0.9, 'negociar': 1.0,
                'desconto': 1.0, 'abatimento': 0.8, 'facilitar': 0.7, 'ajuda': 0.6,
                'dificuldade': 0.8, 'problema': 0.7, 'apertado': 0.8, 'quebrar_galho': 0.9
            },
            context_triggers=['nÃ£o_consigo', 'difÃ­cil', 'sem_dinheiro', 'ajudar'],
            intent_weights={'negociacao_parcelamento': 0.7, 'negociacao_desconto': 0.3},
            emotional_indicators={'frustrado': 0.6, 'urgente': 0.4},
            confidence_modifiers={'pedido': 1.0, 'sugestao': 0.8}
        )
        
        return patterns
    
    def _build_semantic_vectors(self) -> Dict[str, Dict[str, float]]:
        """ğŸ”¬ CONSTRUIR VETORES SEMÃ‚NTICOS BRASILEIROS ULTRA AVANÃ‡ADOS"""
        return {
            # Vetores semÃ¢nticos para palavras de cobranÃ§a
            'fatura': {
                'conta': 0.95, 'boleto': 0.90, 'cobranÃ§a': 0.85, 'dÃ©bito': 0.80,
                'documento': 0.75, 'papel': 0.70, 'segunda_via': 0.95, 'cÃ³pia': 0.60
            },
            'pagar': {
                'quitar': 0.90, 'saldar': 0.85, 'liquidar': 0.80, 'acertar': 0.75,
                'resolver': 0.70, 'transferir': 0.65, 'depositar': 0.60
            },
            'quanto': {
                'valor': 0.95, 'preÃ§o': 0.90, 'custo': 0.85, 'total': 0.80,
                'dinheiro': 0.75, 'grana': 0.80, 'real': 0.70
            },
            'quando': {
                'data': 0.90, 'dia': 0.85, 'prazo': 0.95, 'vencimento': 0.95,
                'tempo': 0.80, 'atÃ©': 0.75, 'hora': 0.70
            },
            'problema': {
                'dificuldade': 0.90, 'complicaÃ§Ã£o': 0.85, 'erro': 0.80,
                'confusÃ£o': 0.75, 'encrenca': 0.85, 'pepino': 0.80
            }
        }
    
    def _build_intent_similarity_matrix(self) -> Dict[str, Dict[str, float]]:
        """ğŸ¯ MATRIZ DE SIMILARIDADE ENTRE INTENÃ‡Ã•ES"""
        return {
            'fatura_solicitar': {
                'fatura_valor': 0.7, 'fatura_vencimento': 0.6, 'pagamento_confirmacao': 0.4,
                'negociacao_parcelamento': 0.3, 'informacao_conta': 0.8
            },
            'fatura_valor': {
                'fatura_solicitar': 0.7, 'fatura_vencimento': 0.5, 'pagamento_confirmacao': 0.6,
                'negociacao_parcelamento': 0.7, 'negociacao_desconto': 0.5
            },
            'negociacao_parcelamento': {
                'negociacao_desconto': 0.8, 'pagamento_dificuldade': 0.9, 'fatura_valor': 0.6
            },
            'pagamento_confirmacao': {
                'reclamacao_valor_incorreto': 0.5, 'fatura_valor': 0.4, 'fatura_solicitar': 0.3
            }
        }
    
    def _build_relationship_graph(self) -> Dict[str, List[str]]:
        """ğŸ•¸ï¸ GRAFO DE RELACIONAMENTOS CONTEXTUAIS"""
        return {
            'financial_entities': ['valor', 'dinheiro', 'real', 'centavo', 'pagar', 'dever'],
            'temporal_entities': ['quando', 'dia', 'data', 'prazo', 'vencimento', 'atÃ©'],
            'document_entities': ['conta', 'boleto', 'fatura', 'papel', 'documento', 'cÃ³pia'],
            'negotiation_entities': ['parcelar', 'dividir', 'acordo', 'desconto', 'facilitar'],
            'emotional_entities': ['problema', 'dificuldade', 'urgente', 'chateado', 'nervoso'],
            'action_entities': ['quero', 'preciso', 'gostaria', 'mandar', 'enviar', 'ver']
        }
    
    def _load_discourse_analyzers(self) -> Dict[str, Any]:
        """ğŸ’¬ ANALISADORES DE DISCURSO ULTRA AVANÃ‡ADOS"""
        return {
            'discourse_markers': {
                'addition': ['tambÃ©m', 'alÃ©m disso', 'e', 'mais', 'ainda'],
                'contrast': ['mas', 'porÃ©m', 'entretanto', 'contudo', 'no entanto'],
                'cause': ['porque', 'pois', 'jÃ¡ que', 'visto que', 'uma vez que'],
                'conclusion': ['entÃ£o', 'portanto', 'assim', 'logo', 'por isso'],
                'sequence': ['primeiro', 'depois', 'em seguida', 'finalmente', 'por Ãºltimo'],
                'emphasis': ['realmente', 'muito', 'bastante', 'extremamente', 'totalmente']
            },
            'pragmatic_markers': {
                'politeness': ['por favor', 'obrigado', 'desculpa', 'com licenÃ§a'],
                'urgency': ['urgente', 'rÃ¡pido', 'agora', 'imediatamente', 'jÃ¡'],
                'uncertainty': ['acho', 'talvez', 'pode ser', 'nÃ£o tenho certeza'],
                'emphasis': ['realmente', 'certamente', 'definitivamente', 'com certeza']
            }
        }
    
    def _build_pragmatic_engine(self) -> Dict[str, Any]:
        """ğŸ§  ENGINE DE INFERÃŠNCIA PRAGMÃTICA ULTRA AVANÃ‡ADA"""
        return {
            'implicature_rules': {
                # Se diz "jÃ¡ paguei MAS ainda aparece" = reclama valor incorreto
                'payment_but_still_charged': {
                    'pattern': r'(jÃ¡.*pagu|quitei|paguei).*(mas|porÃ©m|ainda|continua)',
                    'inference': 'reclamacao_valor_incorreto',
                    'confidence': 0.9
                },
                # Se pergunta valor E prazo = quer informaÃ§Ãµes completas
                'value_and_deadline': {
                    'pattern': r'(quanto.*devo).*(quando.*vence|prazo)',
                    'inference': 'multiple_intents',
                    'confidence': 0.8
                },
                # Se diz que nÃ£o consegue pagar = quer negociar
                'cannot_pay': {
                    'pattern': r'nÃ£o.*(consigo|posso).*(pagar|quitar)',
                    'inference': 'negociacao_parcelamento',
                    'confidence': 0.85
                }
            },
            'contextual_inference': {
                # InferÃªncias baseadas no contexto da conversa
                'follow_up_questions': {
                    'after_invoice_request': ['fatura_valor', 'fatura_vencimento'],
                    'after_negotiation': ['confirmacao', 'negacao', 'duvida'],
                    'after_payment_info': ['pagamento_confirmacao']
                }
            }
        }
    
    def _build_coherence_analyzer(self) -> Dict[str, Any]:
        """ğŸ”— ANALISADOR DE COERÃŠNCIA CONTEXTUAL ULTRA AVANÃ‡ADO"""
        return {
            'coherence_rules': {
                'topic_continuity': {
                    'same_topic': 1.0,      # Mesma intenÃ§Ã£o que anterior
                    'related_topic': 0.8,   # IntenÃ§Ã£o relacionada
                    'topic_shift': 0.4,     # MudanÃ§a de assunto
                    'random_topic': 0.1     # Assunto totalmente aleatÃ³rio
                },
                'temporal_coherence': {
                    'logical_sequence': 1.0,    # SequÃªncia lÃ³gica
                    'acceptable_jump': 0.7,     # Salto aceitÃ¡vel
                    'confusing_sequence': 0.3   # SequÃªncia confusa
                }
            },
            'context_memory_window': 5,  # Quantas mensagens anteriores considerar
            'coherence_threshold': 0.6   # Limite mÃ­nimo de coerÃªncia
        }
    
    def _build_multi_layer_processors(self) -> List[Dict[str, Any]]:
        """ğŸ›ï¸ PROCESSADORES MULTI-CAMADAS ULTRA AVANÃ‡ADOS"""
        return [
            {
                'layer': 'lexical',
                'processor': 'word_level_analysis',
                'weight': 0.2,
                'functions': ['tokenization', 'pos_tagging', 'lemmatization']
            },
            {
                'layer': 'syntactic', 
                'processor': 'phrase_level_analysis',
                'weight': 0.3,
                'functions': ['phrase_detection', 'dependency_parsing']
            },
            {
                'layer': 'semantic',
                'processor': 'meaning_level_analysis', 
                'weight': 0.3,
                'functions': ['semantic_similarity', 'concept_mapping']
            },
            {
                'layer': 'pragmatic',
                'processor': 'context_level_analysis',
                'weight': 0.2,
                'functions': ['pragmatic_inference', 'discourse_analysis']
            }
        ]
    
    def _build_fallback_system(self) -> Dict[str, Any]:
        """ğŸ›¡ï¸ SISTEMA DE FALLBACK INTELIGENTE MULTI-CAMADAS"""
        return {
            'fallback_levels': [
                {
                    'level': 1,
                    'name': 'semantic_similarity',
                    'method': 'find_closest_semantic_match',
                    'threshold': 0.6
                },
                {
                    'level': 2, 
                    'name': 'keyword_extraction',
                    'method': 'extract_key_concepts',
                    'threshold': 0.4
                },
                {
                    'level': 3,
                    'name': 'pattern_matching',
                    'method': 'fuzzy_pattern_match', 
                    'threshold': 0.3
                },
                {
                    'level': 4,
                    'name': 'conversational_context',
                    'method': 'infer_from_conversation',
                    'threshold': 0.2
                },
                {
                    'level': 5,
                    'name': 'intelligent_guess',
                    'method': 'make_educated_guess',
                    'threshold': 0.1
                }
            ]
        }
    
    def _build_dynamic_generator(self) -> Dict[str, Any]:
        """ğŸ­ GERADOR DINÃ‚MICO DE RESPOSTAS ULTRA INTELIGENTE"""
        return {
            'response_templates': {
                'high_confidence': "âœ… **{emotion_marker}** {action_confirmation} {specifics}",
                'medium_confidence': "ğŸ¤” **{understanding}** {clarification_request}",
                'low_confidence': "â“ **{confusion_acknowledgment}** {help_request}",
                'contextual': "ğŸ¯ **{context_reference}** {personalized_response}"
            },
            'emotion_markers': {
                'urgente': ['URGENTE!', 'RAPIDINHO!', 'AGORA MESMO!'],
                'frustrado': ['CALMA!', 'ENTENDO!', 'VAMOS RESOLVER!'],
                'neutro': ['PERFEITO!', 'BELEZA!', 'CERTO!'],
                'satisfeito': ['Ã“TIMO!', 'EXCELENTE!', 'SHOW!']
            },
            'personalization_factors': [
                'conversation_history', 'emotional_state', 'communication_style',
                'previous_intents', 'response_patterns', 'user_preferences'
            ]
        }
    
    # ================================
    # ğŸš€ MÃ‰TODOS ULTRA MEGA AVANÃ‡ADOS - NÃVEL CHATGPT GIGANTEMENTE FODA
    # ================================
    
    def _get_or_create_conversation_memory(self, phone: str) -> ConversationMemory:
        """ğŸ§  OBTER OU CRIAR MEMÃ“RIA ULTRA AVANÃ‡ADA"""
        if phone not in self.conversation_memories:
            self.conversation_memories[phone] = ConversationMemory()
        return self.conversation_memories[phone]
    
    def _ultra_advanced_normalize_text(self, text: str) -> str:
        """ğŸš€ NORMALIZAÃ‡ÃƒO ULTRA MEGA AVANÃ‡ADA"""
        # Usar o mÃ©todo existente mas com melhorias
        normalized = self._super_normalize_text(text)
        
        # Adicionar anÃ¡lises extras ultra avanÃ§adas
        normalized = self._apply_phonetic_corrections(normalized)
        normalized = self._fix_cognitive_errors(normalized)
        normalized = self._standardize_brazilian_expressions(normalized)
        
        return normalized
    
    def _apply_phonetic_corrections(self, text: str) -> str:
        """ğŸ”Š CORREÃ‡Ã•ES FONÃ‰TICAS ULTRA AVANÃ‡ADAS"""
        phonetic_corrections = {
            # CorreÃ§Ãµes baseadas em como as pessoas falam
            r'\b(di)\b': 'de',  # "di manhÃ£" -> "de manhÃ£"
            r'\b(nu)\b': 'no',  # "nu banco" -> "no banco"
            r'\b(du)\b': 'do',  # "du cliente" -> "do cliente"
            r'\b(ma)\b': 'mas', # "ma nÃ£o" -> "mas nÃ£o"
            r'\b(qui)\b': 'que', # "qui dia" -> "que dia"
            r'\b(cumÃ©)\b': 'como Ã©', # "cumÃ© que" -> "como Ã© que"
            r'\b(ocÃª)\b': 'vocÃª',    # "ocÃª tem" -> "vocÃª tem"
            r'\b(seje)\b': 'seja',   # "seje o que" -> "seja o que"
        }
        
        for pattern, replacement in phonetic_corrections.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text
    
    def _fix_cognitive_errors(self, text: str) -> str:
        """ğŸ§  CORRIGIR ERROS COGNITIVOS E DE RACIOCÃNIO"""
        cognitive_fixes = {
            # Erros de lÃ³gica temporal
            r'(ontem.*amanha|amanha.*ontem)': 'ontem ou amanhÃ£',
            # ContradiÃ§Ãµes Ã³bvias
            r'(nÃ£o.*mas.*sim|sim.*mas.*nÃ£o)': 'talvez',
            # ConfusÃµes de pessoa
            r'(vocÃª.*eu.*pagar|eu.*vocÃª.*pagar)': 'preciso pagar',
        }
        
        for pattern, replacement in cognitive_fixes.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text
    
    def _standardize_brazilian_expressions(self, text: str) -> str:
        """ğŸ‡§ğŸ‡· PADRONIZAR EXPRESSÃ•ES TIPICAMENTE BRASILEIRAS"""
        expressions = {
            r'(tÃ¡.*ligado|sacou|entendeu)': 'entende',
            r'(massa|show|da.*hora)': 'bom',
            r'(trampo|labuta)': 'trabalho',
            r'(grana|din.*din|money)': 'dinheiro',
            r'(mina|mano|brother)': 'pessoa',
            r'(rolÃª|role)': 'situaÃ§Ã£o',
        }
        
        for pattern, replacement in expressions.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text
    
    async def _perform_multi_layer_analysis(self, text: str) -> Dict[str, Any]:
        """ğŸ›ï¸ ANÃLISE MULTI-CAMADAS ULTRA PROFUNDA"""
        analysis = {
            'lexical': self._analyze_lexical_layer(text),
            'syntactic': self._analyze_syntactic_layer(text),
            'semantic': self._analyze_semantic_layer(text),
            'pragmatic': self._analyze_pragmatic_layer(text)
        }
        
        # Calcular score agregado
        analysis['overall_complexity'] = sum(
            layer['complexity_score'] * processor['weight'] 
            for layer, processor in zip(analysis.values(), self.multi_layer_processors)
        )
        
        return analysis
    
    def _analyze_lexical_layer(self, text: str) -> Dict[str, Any]:
        """ğŸ“ ANÃLISE LEXICAL ULTRA PROFUNDA"""
        words = text.split()
        
        return {
            'word_count': len(words),
            'avg_word_length': sum(len(w) for w in words) / len(words) if words else 0,
            'complexity_score': min(len(words) * 0.1, 1.0),
            'rare_words': [w for w in words if len(w) > 8],
            'simple_words': [w for w in words if len(w) <= 4]
        }
    
    def _analyze_syntactic_layer(self, text: str) -> Dict[str, Any]:
        """ğŸ”— ANÃLISE SINTÃTICA ULTRA PROFUNDA"""
        # Detectar estruturas sintÃ¡ticas
        has_questions = bool(re.search(r'\?', text))
        has_subordinate = bool(re.search(r'\b(que|se|quando|onde|como)\b', text))
        has_coordination = bool(re.search(r'\b(e|mas|ou|porÃ©m)\b', text))
        
        complexity = 0.3
        if has_questions: complexity += 0.2
        if has_subordinate: complexity += 0.3
        if has_coordination: complexity += 0.2
        
        return {
            'has_questions': has_questions,
            'has_subordinate_clauses': has_subordinate,
            'has_coordination': has_coordination,
            'complexity_score': min(complexity, 1.0)
        }
    
    def _analyze_semantic_layer(self, text: str) -> Dict[str, Any]:
        """ğŸ§  ANÃLISE SEMÃ‚NTICA ULTRA PROFUNDA"""
        semantic_clusters = []
        cluster_scores = {}
        
        # Analisar proximidade semÃ¢ntica com nossos clusters
        for cluster_name, words in self.contextual_relationship_graph.items():
            score = 0
            for word in words:
                if word in text.lower():
                    score += 1
            
            if score > 0:
                semantic_clusters.append(cluster_name)
                cluster_scores[cluster_name] = score / len(words)
        
        return {
            'semantic_clusters': semantic_clusters,
            'cluster_scores': cluster_scores,
            'complexity_score': min(len(semantic_clusters) * 0.2, 1.0),
            'semantic_density': sum(cluster_scores.values()) / max(len(cluster_scores), 1)
        }
    
    def _analyze_pragmatic_layer(self, text: str) -> Dict[str, Any]:
        """ğŸ’­ ANÃLISE PRAGMÃTICA ULTRA PROFUNDA"""
        pragmatic_elements = {}
        
        # Detectar elementos pragmÃ¡ticos
        for marker_type, markers in self.discourse_analyzers['pragmatic_markers'].items():
            found_markers = [m for m in markers if m in text.lower()]
            if found_markers:
                pragmatic_elements[marker_type] = found_markers
        
        return {
            'pragmatic_elements': pragmatic_elements,
            'complexity_score': min(len(pragmatic_elements) * 0.25, 1.0),
            'pragmatic_richness': len(pragmatic_elements)
        }
    
    async def _perform_semantic_analysis(self, text: str, memory: ConversationMemory) -> Dict[str, Any]:
        """ğŸ”¬ ANÃLISE SEMÃ‚NTICA MEGA ULTRA AVANÃ‡ADA"""
        semantic_analysis = {}
        
        # Calcular similaridade semÃ¢ntica com padrÃµes conhecidos
        for pattern_id, pattern in self.semantic_patterns.items():
            similarity = self._calculate_semantic_similarity(text, pattern)
            semantic_analysis[pattern_id] = similarity
        
        # AnÃ¡lise de vetores semÃ¢nticos
        vector_analysis = self._analyze_semantic_vectors(text)
        
        return {
            'pattern_similarities': semantic_analysis,
            'vector_analysis': vector_analysis,
            'best_match': max(semantic_analysis.items(), key=lambda x: x[1]) if semantic_analysis else None,
            'semantic_confidence': max(semantic_analysis.values()) if semantic_analysis else 0.0
        }
    
    def _calculate_semantic_similarity(self, text: str, pattern: SemanticPattern) -> float:
        """ğŸ“ CALCULAR SIMILARIDADE SEMÃ‚NTICA ULTRA PRECISA"""
        similarity_score = 0.0
        total_weight = 0.0
        
        # Analisar vetores semÃ¢nticos
        for concept, weight in pattern.semantic_vectors.items():
            if concept in text.lower():
                similarity_score += weight
            total_weight += weight
        
        # Normalizar score
        if total_weight > 0:
            similarity_score = similarity_score / total_weight
        
        # Boost por triggers contextuais
        for trigger in pattern.context_triggers:
            if trigger in text.lower():
                similarity_score += 0.1
        
        return min(similarity_score, 1.0)
    
    def _analyze_semantic_vectors(self, text: str) -> Dict[str, float]:
        """ğŸ§® ANÃLISE DE VETORES SEMÃ‚NTICOS"""
        vector_scores = {}
        
        for main_concept, related_concepts in self.brazilian_semantic_vectors.items():
            if main_concept in text.lower():
                vector_scores[main_concept] = 1.0
                
                # Adicionar conceitos relacionados
                for related, similarity in related_concepts.items():
                    if related in text.lower():
                        vector_scores[related] = similarity
        
        return vector_scores
    
    async def _perform_pragmatic_analysis(self, text: str, memory: ConversationMemory) -> Dict[str, Any]:
        """ğŸ­ ANÃLISE PRAGMÃTICA MEGA ULTRA AVANÃ‡ADA"""
        pragmatic_inferences = {}
        
        # Aplicar regras de implicatura
        for rule_name, rule in self.pragmatic_inference_engine['implicature_rules'].items():
            if re.search(rule['pattern'], text, re.IGNORECASE):
                pragmatic_inferences[rule_name] = {
                    'inference': rule['inference'],
                    'confidence': rule['confidence']
                }
        
        # AnÃ¡lise contextual baseada na conversa anterior
        contextual_inferences = self._analyze_conversational_context(text, memory)
        
        return {
            'implicatures': pragmatic_inferences,
            'contextual_inferences': contextual_inferences,
            'pragmatic_confidence': max(
                [inf['confidence'] for inf in pragmatic_inferences.values()] + [0.0]
            )
        }
    
    def _analyze_conversational_context(self, text: str, memory: ConversationMemory) -> Dict[str, Any]:
        """ğŸ’¬ ANÃLISE DE CONTEXTO CONVERSACIONAL ULTRA PROFUNDA"""
        inferences = {}
        
        # Analisar padrÃ£o baseado na Ãºltima intenÃ§Ã£o
        if memory.intent_history:
            last_intent, confidence, timestamp = memory.intent_history[-1]
            
            # Inferir follow-ups baseados na intenÃ§Ã£o anterior
            follow_ups = self.pragmatic_inference_engine['contextual_inference']['follow_up_questions']
            if last_intent in follow_ups:
                for possible_intent in follow_ups[last_intent]:
                    inferences[f'follow_up_{possible_intent}'] = confidence * 0.7
        
        return inferences
    
    async def _extract_ultra_advanced_entities(self, text: str, semantic_analysis: Dict[str, Any]) -> List[ExtractedEntity]:
        """ğŸ¯ EXTRAÃ‡ÃƒO ULTRA AVANÃ‡ADA DE ENTIDADES COM CONTEXTO SEMÃ‚NTICO"""
        entities = []
        
        # Usar mÃ©todo existente como base
        base_entities = self._extract_all_entities(text)
        
        # Enriquecer com anÃ¡lise semÃ¢ntica
        for entity in base_entities:
            # Calcular peso semÃ¢ntico
            semantic_weight = 1.0
            if semantic_analysis.get('vector_analysis'):
                for concept, score in semantic_analysis['vector_analysis'].items():
                    if concept in entity.value.lower():
                        semantic_weight = max(semantic_weight, score)
            
            # Adicionar alternativas baseadas em similaridade
            alternatives = self._find_entity_alternatives(entity, semantic_analysis)
            
            # Criar entidade enriquecida
            ultra_entity = ExtractedEntity(
                type=entity.type,
                value=entity.value,
                confidence=entity.confidence,
                context=entity.context,
                semantic_weight=semantic_weight,
                alternatives=alternatives,
                relationships=self._find_entity_relationships(entity, text)
            )
            
            entities.append(ultra_entity)
        
        return entities
    
    def _find_entity_alternatives(self, entity: ExtractedEntity, semantic_analysis: Dict[str, Any]) -> List[str]:
        """ğŸ” ENCONTRAR ALTERNATIVAS SEMÃ‚NTICAS PARA ENTIDADES"""
        alternatives = []
        
        if entity.type == 'valores_monetarios':
            alternatives = ['valor', 'quantia', 'dinheiro', 'preÃ§o', 'custo']
        elif entity.type == 'datas':
            alternatives = ['prazo', 'vencimento', 'data', 'dia', 'quando']
        
        return alternatives
    
    def _find_entity_relationships(self, entity: ExtractedEntity, text: str) -> Dict[str, float]:
        """ğŸ•¸ï¸ ENCONTRAR RELACIONAMENTOS ENTRE ENTIDADES"""
        relationships = {}
        
        # Analisar proximidade com outras palavras-chave
        for cluster_name, words in self.contextual_relationship_graph.items():
            for word in words:
                if word in text.lower() and word != entity.value.lower():
                    relationships[word] = 0.8  # Score de relacionamento
        
        return relationships
    
    async def _analyze_ultra_emotion(self, text: str, memory: ConversationMemory) -> str:
        """ğŸ˜Š ANÃLISE EMOCIONAL ULTRA AVANÃ‡ADA COM MEMÃ“RIA"""
        # Usar anÃ¡lise existente como base
        base_emotion = self._analyze_emotion(text)
        
        # Enriquecer com contexto de memÃ³ria emocional
        if memory.emotional_journey:
            # Considerar padrÃ£o emocional histÃ³rico
            recent_emotions = [emotion for emotion, score, timestamp in memory.emotional_journey[-3:]]
            
            # Se hÃ¡ padrÃ£o de frustraÃ§Ã£o crescente
            if recent_emotions.count('frustrado') >= 2:
                if base_emotion in ['neutro', 'confuso']:
                    base_emotion = 'frustrado'  # Inferir frustraÃ§Ã£o continuada
        
        # Detectar escalation emocional
        emotional_escalation = self._detect_emotional_escalation(text)
        if emotional_escalation:
            if base_emotion == 'frustrado':
                base_emotion = 'muito_frustrado'  # Nova categoria
            elif base_emotion == 'urgente':
                base_emotion = 'extremamente_urgente'  # Nova categoria
        
        return base_emotion
    
    def _detect_emotional_escalation(self, text: str) -> bool:
        """ğŸ“ˆ DETECTAR ESCALATION EMOCIONAL"""
        escalation_markers = [
            r'(muito|extremamente|super|ultra).*(chateado|irritado)',
            r'(nÃ£o.*aguentar|nÃ£o.*suportar)',
            r'(absurdo|ridÃ­culo|inaceitÃ¡vel)',
            r'[!]{3,}',  # MÃºltiplas exclamaÃ§Ãµes
            r'[?!]{2,}',  # Mistura de ? e !
        ]
        
        return any(re.search(pattern, text, re.IGNORECASE) for pattern in escalation_markers)
    
    async def _analyze_ultra_temporal_context(self, text: str, memory: ConversationMemory) -> str:
        """â° ANÃLISE TEMPORAL ULTRA AVANÃ‡ADA"""
        base_temporal = self._analyze_temporal_context(text)
        
        # Enriquecer com anÃ¡lise de urgÃªncia temporal
        urgency_indicators = {
            'imediato': ['agora', 'jÃ¡', 'imediatamente', 'urgente'],
            'hoje': ['hoje', 'hj', 'ainda hoje'],
            'breve': ['logo', 'em breve', 'rapidinho'],
            'futuro_proximo': ['amanhÃ£', 'essa semana', 'uns dias'],
            'futuro_distante': ['mÃªs que vem', 'ano que vem', 'mais tarde']
        }
        
        for urgency_level, indicators in urgency_indicators.items():
            if any(indicator in text.lower() for indicator in indicators):
                return f"{base_temporal}_{urgency_level}"
        
        return base_temporal
    
    async def _analyze_ultra_negation(self, text: str) -> Dict[str, Any]:
        """âŒ ANÃLISE ULTRA AVANÃ‡ADA DE NEGAÃ‡ÃƒO"""
        has_basic_negation = self._detect_negation(text)
        
        # AnÃ¡lise mais sofisticada de tipos de negaÃ§Ã£o
        negation_types = {
            'absolute': r'\b(nunca|jamais|de jeito nenhum)\b',
            'partial': r'\b(nÃ£o muito|meio que nÃ£o|acho que nÃ£o)\b',
            'conditional': r'\b(nÃ£o se|sÃ³ nÃ£o|a nÃ£o ser)\b',
            'emphatic': r'\b(de forma alguma|nem pensar|que nada)\b'
        }
        
        detected_types = []
        for neg_type, pattern in negation_types.items():
            if re.search(pattern, text, re.IGNORECASE):
                detected_types.append(neg_type)
        
        return {
            'has_negation': has_basic_negation,
            'negation_types': detected_types,
            'negation_strength': len(detected_types) / len(negation_types)
        }
    
    async def _analyze_ultra_contextual_intent(
        self, text: str, entities: List[ExtractedEntity], emotion: str, 
        temporal: str, negation: Dict, memory: ConversationMemory, 
        semantic_analysis: Dict, pragmatic_analysis: Dict
    ) -> ContextualIntent:
        """ğŸ§  ANÃLISE ULTRA MEGA AVANÃ‡ADA DE INTENÃ‡ÃƒO CONTEXTUAL"""
        
        # Usar anÃ¡lise base existente
        base_intent_analysis = self._analyze_contextual_intent(
            text, entities, emotion, temporal, negation.get('has_negation', False), memory
        )
        
        # ENRIQUECER COM ANÃLISES ULTRA AVANÃ‡ADAS
        
        # 1. Boost semÃ¢ntico baseado na melhor correspondÃªncia
        if semantic_analysis.get('best_match'):
            pattern_id, similarity_score = semantic_analysis['best_match']
            if similarity_score > 0.7:
                # Aplicar boost baseado no padrÃ£o semÃ¢ntico
                if 'fatura' in pattern_id:
                    base_intent_analysis.confidence += 0.2
                elif 'valor' in pattern_id:
                    base_intent_analysis.confidence += 0.15
        
        # 2. Boost pragmÃ¡tico baseado em implicaturas
        pragmatic_confidence = pragmatic_analysis.get('pragmatic_confidence', 0)
        base_intent_analysis.confidence += pragmatic_confidence * 0.1
        
        # 3. Calcular similaridade semÃ¢ntica com intenÃ§Ãµes conhecidas
        semantic_similarity = self._calculate_intent_semantic_similarity(
            base_intent_analysis.intent, semantic_analysis
        )
        
        # 4. Analisar alternativas de intenÃ§Ã£o
        alternative_intents = self._calculate_alternative_intents(
            text, semantic_analysis, pragmatic_analysis
        )
        
        # 5. Detectar clusters semÃ¢nticos
        semantic_clusters = semantic_analysis.get('pattern_similarities', {}).keys()
        
        # 6. Analisar marcadores de discurso
        discourse_markers = self._extract_discourse_markers(text)
        
        # 7. InferÃªncia pragmÃ¡tica ultra avanÃ§ada
        pragmatic_inference = self._calculate_pragmatic_inference(
            base_intent_analysis, pragmatic_analysis, memory
        )
        
        # Criar intenÃ§Ã£o contextual ultra enriquecida
        ultra_intent = ContextualIntent(
            intent=base_intent_analysis.intent,
            confidence=min(base_intent_analysis.confidence, 1.0),
            entities=entities,
            temporal_context=temporal,
            emotional_state=emotion,
            negation=negation.get('has_negation', False),
            multiple_intents=base_intent_analysis.multiple_intents,
            
            # CAMPOS ULTRA AVANÃ‡ADOS
            semantic_similarity=semantic_similarity,
            contextual_coherence=0.0,  # SerÃ¡ calculado depois
            linguistic_complexity=semantic_analysis.get('semantic_confidence', 0),
            intent_certainty=0.0,  # SerÃ¡ calculado depois
            alternative_intents=alternative_intents,
            semantic_clusters=list(semantic_clusters),
            discourse_markers=discourse_markers,
            pragmatic_inference=pragmatic_inference
        )
        
        return ultra_intent
    
    def _calculate_intent_semantic_similarity(self, intent: IntentType, semantic_analysis: Dict) -> float:
        """ğŸ“ CALCULAR SIMILARIDADE SEMÃ‚NTICA DA INTENÃ‡ÃƒO"""
        intent_key = intent.value
        similarity_matrix = self.intent_similarity_matrix
        
        if intent_key in similarity_matrix:
            # Calcular mÃ©dia das similaridades com outras intenÃ§Ãµes detectadas
            similarities = []
            for related_intent, similarity in similarity_matrix[intent_key].items():
                if any(related_intent in cluster for cluster in semantic_analysis.get('pattern_similarities', {})):
                    similarities.append(similarity)
            
            return sum(similarities) / len(similarities) if similarities else 0.5
        
        return 0.5  # Default
    
    def _calculate_alternative_intents(self, text: str, semantic_analysis: Dict, pragmatic_analysis: Dict) -> List[Tuple[IntentType, float]]:
        """ğŸ¯ CALCULAR INTENÃ‡Ã•ES ALTERNATIVAS"""
        alternatives = []
        
        # Baseado em anÃ¡lise semÃ¢ntica
        for pattern_id, similarity in semantic_analysis.get('pattern_similarities', {}).items():
            if similarity > 0.5:
                if 'fatura' in pattern_id:
                    alternatives.append((IntentType.FATURA_SOLICITAR, similarity))
                elif 'valor' in pattern_id:
                    alternatives.append((IntentType.FATURA_VALOR, similarity))
                elif 'negociacao' in pattern_id:
                    alternatives.append((IntentType.NEGOCIACAO_PARCELAMENTO, similarity))
        
        # Remover duplicatas e ordenar por confianÃ§a
        alternatives = list(set(alternatives))
        alternatives.sort(key=lambda x: x[1], reverse=True)
        
        return alternatives[:3]  # Top 3 alternativas
    
    def _extract_discourse_markers(self, text: str) -> List[str]:
        """ğŸ’¬ EXTRAIR MARCADORES DE DISCURSO"""
        markers = []
        
        for marker_type, marker_list in self.discourse_analyzers['discourse_markers'].items():
            for marker in marker_list:
                if marker in text.lower():
                    markers.append(f"{marker_type}:{marker}")
        
        return markers
    
    def _calculate_pragmatic_inference(self, intent: ContextualIntent, pragmatic_analysis: Dict, memory: ConversationMemory) -> Dict[str, float]:
        """ğŸ­ CALCULAR INFERÃŠNCIA PRAGMÃTICA"""
        inferences = {}
        
        # InferÃªncias baseadas em implicaturas
        for implicature_name, implicature_data in pragmatic_analysis.get('implicatures', {}).items():
            inferences[implicature_name] = implicature_data['confidence']
        
        # InferÃªncias contextuais
        contextual_infs = pragmatic_analysis.get('contextual_inferences', {})
        inferences.update(contextual_infs)
        
        return inferences
    
    async def _analyze_contextual_coherence(self, intent: ContextualIntent, memory: ConversationMemory) -> float:
        """ğŸ”— ANALISAR COERÃŠNCIA CONTEXTUAL"""
        if not memory.intent_history:
            return 0.8  # Primeira mensagem tem coerÃªncia neutra
        
        # Pegar Ãºltimas 3 intenÃ§Ãµes
        recent_intents = [intent_data[0] for intent_data in memory.intent_history[-3:]]
        current_intent = intent.intent.value
        
        # Calcular coerÃªncia baseada na matriz de similaridade
        coherence_scores = []
        
        for past_intent in recent_intents:
            if past_intent in self.intent_similarity_matrix:
                if current_intent in self.intent_similarity_matrix[past_intent]:
                    coherence_scores.append(self.intent_similarity_matrix[past_intent][current_intent])
                else:
                    coherence_scores.append(0.3)  # Baixa coerÃªncia para intenÃ§Ãµes nÃ£o relacionadas
        
        return sum(coherence_scores) / len(coherence_scores) if coherence_scores else 0.5
    
    async def _calculate_intent_certainty(self, intent: ContextualIntent, linguistic_analysis: Dict) -> float:
        """âœ… CALCULAR CERTEZA DA INTENÃ‡ÃƒO"""
        certainty_factors = []
        
        # Fator 1: ConfianÃ§a base da intenÃ§Ã£o
        certainty_factors.append(intent.confidence)
        
        # Fator 2: Similaridade semÃ¢ntica
        certainty_factors.append(intent.semantic_similarity)
        
        # Fator 3: CoerÃªncia contextual
        certainty_factors.append(intent.contextual_coherence)
        
        # Fator 4: Complexidade linguÃ­stica (menos complexo = mais certo)
        linguistic_certainty = 1.0 - linguistic_analysis.get('overall_complexity', 0.5)
        certainty_factors.append(linguistic_certainty)
        
        # Fator 5: PresenÃ§a de entidades relevantes
        entity_certainty = min(len(intent.entities) * 0.2, 1.0)
        certainty_factors.append(entity_certainty)
        
        # Calcular mÃ©dia ponderada
        weights = [0.3, 0.2, 0.2, 0.15, 0.15]  # Soma = 1.0
        weighted_certainty = sum(factor * weight for factor, weight in zip(certainty_factors, weights))
        
        return min(weighted_certainty, 1.0)
    
    async def _update_ultra_conversation_memory(self, phone: str, intent: ContextualIntent, text: str, linguistic_analysis: Dict):
        """ğŸ§  ATUALIZAR MEMÃ“RIA ULTRA AVANÃ‡ADA"""
        memory = self.conversation_memories[phone]
        
        # Atualizar histÃ³rico de intenÃ§Ãµes
        memory.intent_history.append((
            intent.intent.value, 
            intent.confidence, 
            datetime.now()
        ))
        
        # Atualizar jornada emocional
        memory.emotional_journey.append((
            intent.emotional_state,
            intent.confidence,
            datetime.now()
        ))
        
        # Atualizar padrÃµes de conversaÃ§Ã£o
        memory.conversation_patterns.append(text[:100])  # Primeiros 100 chars
        
        # Detectar mudanÃ§as de contexto
        if len(memory.intent_history) > 1:
            last_intent = memory.intent_history[-2][0]
            if intent.intent.value != last_intent:
                if intent.contextual_coherence < 0.4:  # MudanÃ§a abrupta
                    memory.context_switches.append(datetime.now())
        
        # Atualizar dados de aprendizado
        memory.learning_data['total_messages'] = memory.learning_data.get('total_messages', 0) + 1
        memory.learning_data['avg_confidence'] = (
            memory.learning_data.get('avg_confidence', 0.5) + intent.confidence
        ) / 2
        
        # Manter apenas Ãºltimos 50 registros de cada tipo
        memory.intent_history = memory.intent_history[-50:]
        memory.emotional_journey = memory.emotional_journey[-50:]
        memory.conversation_patterns = memory.conversation_patterns[-50:]
        memory.context_switches = memory.context_switches[-20:]
    
    async def _learn_from_interaction(self, phone: str, intent: ContextualIntent, semantic_analysis: Dict):
        """ğŸ“ APRENDER A PARTIR DA INTERAÃ‡ÃƒO"""
        # Armazenar padrÃµes bem-sucedidos para aprendizado futuro
        if intent.confidence > 0.8:
            pattern_key = f"{intent.intent.value}_{intent.emotional_state}"
            
            if pattern_key not in self.pattern_learning_db:
                self.pattern_learning_db[pattern_key] = []
            
            # Armazenar caracterÃ­sticas da mensagem bem entendida
            learning_pattern = {
                'semantic_clusters': intent.semantic_clusters,
                'entities_count': len(intent.entities),
                'discourse_markers': intent.discourse_markers,
                'confidence': intent.confidence,
                'timestamp': datetime.now()
            }
            
            self.pattern_learning_db[pattern_key].append(learning_pattern)
            
            # Manter apenas Ãºltimos 20 padrÃµes por tipo
            self.pattern_learning_db[pattern_key] = self.pattern_learning_db[pattern_key][-20:]
    
    async def _generate_ultra_contextual_response(
        self, phone: str, intent: ContextualIntent, entities: List[ExtractedEntity], 
        memory: ConversationMemory, semantic_analysis: Dict
    ) -> Dict[str, Any]:
        """ğŸ­ GERAÃ‡ÃƒO ULTRA INTELIGENTE DE RESPOSTA NÃVEL CHATGPT"""
        
        # Usar gerador existente como base
        base_response = await self._generate_contextual_response(phone, intent, entities, {})
        
        # ENRIQUECER COM INTELIGÃŠNCIA ULTRA AVANÃ‡ADA
        
        # 1. PersonalizaÃ§Ã£o baseada em memÃ³ria
        personalization = self._generate_personalized_elements(memory, intent)
        
        # 2. AdaptaÃ§Ã£o baseada em certeza
        certainty_adaptation = self._adapt_response_for_certainty(intent.intent_certainty)
        
        # 3. ContextualizaÃ§Ã£o semÃ¢ntica
        semantic_context = self._add_semantic_context(semantic_analysis, intent)
        
        # 4. Resposta dinÃ¢mica baseada em padrÃµes aprendidos
        learned_enhancements = self._apply_learned_patterns(intent, memory)
        
        # Gerar resposta ultra contextualizada
        ultra_response_text = self._compose_ultra_response(
            base_response['text'], personalization, certainty_adaptation, 
            semantic_context, learned_enhancements, intent
        )
        
        return {
            'text': ultra_response_text,
            'intent': intent.intent.value,
            'confidence': intent.confidence,
            'entities_detected': len(entities),
            'emotional_state': intent.emotional_state,
            'multiple_intents': len(intent.multiple_intents),
            'context_enhanced': True,
            'response_type': 'ultra_mega_contextual',
            
            # NOVOS CAMPOS ULTRA AVANÃ‡ADOS
            'semantic_similarity': intent.semantic_similarity,
            'contextual_coherence': intent.contextual_coherence,
            'intent_certainty': intent.intent_certainty,
            'personalization_level': len(personalization),
            'semantic_clusters': intent.semantic_clusters,
            'discourse_markers': intent.discourse_markers,
            'ultra_enhanced': True
        }
    
    def _generate_personalized_elements(self, memory: ConversationMemory, intent: ContextualIntent) -> Dict[str, str]:
        """ğŸ‘¤ GERAR ELEMENTOS PERSONALIZADOS"""
        personalization = {}
        
        # Baseado em padrÃ£o emocional
        if memory.emotional_journey:
            recent_emotions = [emotion for emotion, _, _ in memory.emotional_journey[-3:]]
            if recent_emotions.count('frustrado') >= 2:
                personalization['empathy'] = "Eu vejo que vocÃª estÃ¡ passando por uma situaÃ§Ã£o chata"
            elif recent_emotions.count('urgente') >= 2:
                personalization['urgency_ack'] = "Entendo que isso Ã© urgente para vocÃª"
        
        # Baseado em histÃ³rico de intenÃ§Ãµes
        if memory.intent_history:
            common_intents = Counter([intent for intent, _, _ in memory.intent_history])
            most_common = common_intents.most_common(1)[0][0]
            if most_common == 'fatura_solicitar':
                personalization['context'] = "Como sempre, vou buscar sua fatura"
        
        return personalization
    
    def _adapt_response_for_certainty(self, certainty: float) -> Dict[str, str]:
        """âœ… ADAPTAR RESPOSTA BASEADA NA CERTEZA"""
        if certainty > 0.9:
            return {'confidence_marker': '**CERTEZA ABSOLUTA!**', 'action': 'Vou resolver isso AGORA!'}
        elif certainty > 0.7:
            return {'confidence_marker': '**ENTENDI PERFEITAMENTE!**', 'action': 'Vou cuidar disso!'}
        elif certainty > 0.5:
            return {'confidence_marker': '**ACHO QUE ENTENDI!**', 'action': 'Deixe-me confirmar...'}
        else:
            return {'confidence_marker': '**HMMMM...**', 'action': 'Me explica melhor?'}
    
    def _add_semantic_context(self, semantic_analysis: Dict, intent: ContextualIntent) -> Dict[str, str]:
        """ğŸ§  ADICIONAR CONTEXTO SEMÃ‚NTICO"""
        context = {}
        
        if semantic_analysis.get('best_match'):
            pattern_id, score = semantic_analysis['best_match']
            if score > 0.8:
                context['semantic_confidence'] = f"Detectei {int(score*100)}% de certeza"
        
        return context
    
    def _apply_learned_patterns(self, intent: ContextualIntent, memory: ConversationMemory) -> Dict[str, str]:
        """ğŸ“ APLICAR PADRÃ•ES APRENDIDOS"""
        enhancements = {}
        
        pattern_key = f"{intent.intent.value}_{intent.emotional_state}"
        if pattern_key in self.pattern_learning_db:
            patterns = self.pattern_learning_db[pattern_key]
            if patterns:
                # Aplicar insights dos padrÃµes aprendidos
                avg_confidence = sum(p['confidence'] for p in patterns) / len(patterns)
                if avg_confidence > 0.8:
                    enhancements['learned_boost'] = "Baseado no que aprendi com vocÃª"
        
        return enhancements
    
    def _compose_ultra_response(
        self, base_text: str, personalization: Dict, certainty: Dict, 
        semantic: Dict, learned: Dict, intent: ContextualIntent
    ) -> str:
        """ğŸ­ COMPOR RESPOSTA ULTRA AVANÃ‡ADA"""
        
        # ComeÃ§ar com texto base
        response_parts = [base_text]
        
        # Adicionar personalizaÃ§Ã£o
        if personalization.get('empathy'):
            response_parts.insert(0, personalization['empathy'] + ".")
        
        # Adicionar marcador de confianÃ§a
        if certainty.get('confidence_marker'):
            response_parts[0] = response_parts[0].replace(
                response_parts[0].split()[0], 
                certainty['confidence_marker']
            )
        
        # Adicionar contexto semÃ¢ntico se alta confianÃ§a
        if semantic.get('semantic_confidence'):
            response_parts.append(f"\n\nğŸ¯ {semantic['semantic_confidence']} no que vocÃª quis dizer!")
        
        # Adicionar insights aprendidos
        if learned.get('learned_boost'):
            response_parts.append(f"\n\nğŸ§  {learned['learned_boost']}, sei exatamente o que fazer!")
        
        return " ".join(response_parts)
    
    async def _ultra_intelligent_fallback(self, phone: str, text: str, error: Exception) -> Dict[str, Any]:
        """ğŸ›¡ï¸ FALLBACK ULTRA INTELIGENTE MULTI-CAMADAS"""
        
        logger.error(f"ğŸš€ Ativando fallback ultra inteligente para: {text[:50]}... | Erro: {error}")
        
        # Tentar fallbacks em cascata
        for fallback_level in self.intelligent_fallback_system['fallback_levels']:
            try:
                if fallback_level['name'] == 'semantic_similarity':
                    return await self._fallback_semantic_similarity(text, fallback_level['threshold'])
                elif fallback_level['name'] == 'keyword_extraction':
                    return await self._fallback_keyword_extraction(text, fallback_level['threshold'])
                elif fallback_level['name'] == 'pattern_matching':
                    return await self._fallback_pattern_matching(text, fallback_level['threshold'])
                elif fallback_level['name'] == 'conversational_context':
                    return await self._fallback_conversational_context(phone, text, fallback_level['threshold'])
                elif fallback_level['name'] == 'intelligent_guess':
                    return await self._fallback_intelligent_guess(text, fallback_level['threshold'])
                    
            except Exception as fallback_error:
                logger.warning(f"Fallback nÃ­vel {fallback_level['level']} falhou: {fallback_error}")
                continue
        
        # Fallback final de emergÃªncia
        return {
            'text': "ğŸ¤” **NOSSA!** Essa foi difÃ­cil atÃ© para mim! Pode tentar falar de um jeito mais simples? Tipo: 'quero minha conta' ou 'quanto devo'?",
            'intent': 'emergency_fallback',
            'confidence': 0.1,
            'fallback_level': 'emergency',
            'ultra_enhanced': True
        }
    
    async def _fallback_semantic_similarity(self, text: str, threshold: float) -> Dict[str, Any]:
        """ğŸ” FALLBACK POR SIMILARIDADE SEMÃ‚NTICA"""
        # Tentar encontrar padrÃ£o semÃ¢ntico mais prÃ³ximo
        best_match = None
        best_score = 0.0
        
        for pattern_id, pattern in self.semantic_patterns.items():
            score = self._calculate_semantic_similarity(text, pattern)
            if score > best_score and score > threshold:
                best_match = pattern_id
                best_score = score
        
        if best_match:
            intent_mapping = {
                'fatura_semantic': 'fatura_solicitar',
                'valor_semantic': 'fatura_valor',
                'tempo_semantic': 'fatura_vencimento',
                'negociacao_semantic': 'negociacao_parcelamento'
            }
            
            inferred_intent = intent_mapping.get(best_match, 'fatura_solicitar')
            
            return {
                'text': f"ğŸ¯ **ENTENDI PELO CONTEXTO!** VocÃª quer algo relacionado a {inferred_intent.replace('_', ' ')}. Vou ajudar!",
                'intent': inferred_intent,
                'confidence': best_score,
                'fallback_level': 'semantic_similarity',
                'ultra_enhanced': True
            }
        
        raise Exception("Similaridade semÃ¢ntica insuficiente")
    
    async def _fallback_keyword_extraction(self, text: str, threshold: float) -> Dict[str, Any]:
        """ğŸ”‘ FALLBACK POR EXTRAÃ‡ÃƒO DE PALAVRAS-CHAVE"""
        keywords = {
            'fatura': ['conta', 'boleto', 'fatura', 'segunda', 'via', 'papel'],
            'valor': ['quanto', 'valor', 'devo', 'pagar', 'preÃ§o', 'dinheiro'],
            'vencimento': ['quando', 'vence', 'prazo', 'data', 'atÃ©'],
            'negociacao': ['parcelar', 'acordo', 'desconto', 'negociar', 'facilitar']
        }
        
        scores = {}
        for intent, intent_keywords in keywords.items():
            score = sum(1 for keyword in intent_keywords if keyword in text.lower())
            if score > 0:
                scores[intent] = score / len(intent_keywords)
        
        if scores:
            best_intent = max(scores.items(), key=lambda x: x[1])
            if best_intent[1] > threshold:
                return {
                    'text': f"ğŸ” **CAPTEI!** Pelas palavras-chave, vocÃª quer {best_intent[0]}. Ã‰ isso mesmo?",
                    'intent': best_intent[0],
                    'confidence': best_intent[1],
                    'fallback_level': 'keyword_extraction',
                    'ultra_enhanced': True
                }
        
        raise Exception("Palavras-chave insuficientes")
    
    async def _fallback_pattern_matching(self, text: str, threshold: float) -> Dict[str, Any]:
        """ğŸ§© FALLBACK POR CORRESPONDÃŠNCIA DE PADRÃ•ES"""
        # PadrÃµes de emergÃªncia muito bÃ¡sicos
        emergency_patterns = [
            (r'\b(conta|boleto|fatura)\b', 'fatura_solicitar', 0.7),
            (r'\b(quanto|valor)\b', 'fatura_valor', 0.6),
            (r'\b(quando|vence|prazo)\b', 'fatura_vencimento', 0.6),
            (r'\b(paguei|pago)\b', 'pagamento_confirmacao', 0.5),
            (r'\b(parcelar|acordo)\b', 'negociacao_parcelamento', 0.5),
        ]
        
        for pattern, intent, confidence in emergency_patterns:
            if re.search(pattern, text, re.IGNORECASE) and confidence > threshold:
                return {
                    'text': f"ğŸ§© **CONSEGUI ENTENDER!** Pelo padrÃ£o, vocÃª quer {intent.replace('_', ' ')}!",
                    'intent': intent,
                    'confidence': confidence,
                    'fallback_level': 'pattern_matching',
                    'ultra_enhanced': True
                }
        
        raise Exception("Nenhum padrÃ£o corresponde")
    
    async def _fallback_conversational_context(self, phone: str, text: str, threshold: float) -> Dict[str, Any]:
        """ğŸ’­ FALLBACK POR CONTEXTO CONVERSACIONAL"""
        if phone in self.conversation_memories:
            memory = self.conversation_memories[phone]
            if memory.intent_history:
                # Assumir que Ã© follow-up da Ãºltima intenÃ§Ã£o
                last_intent, last_confidence, _ = memory.intent_history[-1]
                
                if last_confidence > threshold:
                    return {
                        'text': f"ğŸ’­ **PELO CONTEXTO!** VocÃª ainda estÃ¡ falando sobre {last_intent.replace('_', ' ')}, nÃ©?",
                        'intent': last_intent,
                        'confidence': last_confidence * 0.8,
                        'fallback_level': 'conversational_context',
                        'ultra_enhanced': True
                    }
        
        raise Exception("Contexto conversacional insuficiente")
    
    async def _fallback_intelligent_guess(self, text: str, threshold: float) -> Dict[str, Any]:
        """ğŸ² FALLBACK POR SUPOSIÃ‡ÃƒO INTELIGENTE"""
        # Se chegou atÃ© aqui, fazer uma suposiÃ§Ã£o educada baseada no contexto de cobranÃ§a
        text_length = len(text.split())
        
        if text_length <= 3:
            # Texto muito curto - provavelmente quer fatura
            guess_intent = 'fatura_solicitar'
            guess_confidence = 0.4
        elif '?' in text:
            # Tem pergunta - provavelmente quer informaÃ§Ã£o (valor ou vencimento)
            guess_intent = 'fatura_valor'
            guess_confidence = 0.3
        else:
            # Default para solicitaÃ§Ã£o de fatura
            guess_intent = 'fatura_solicitar'
            guess_confidence = 0.2
        
        if guess_confidence > threshold:
            return {
                'text': f"ğŸ² **VAMOS TENTAR!** Pelo contexto geral, acho que vocÃª quer {guess_intent.replace('_', ' ')}. Se nÃ£o for isso, me fala 'nÃ£o' que eu entendo outra coisa!",
                'intent': guess_intent,
                'confidence': guess_confidence,
                'fallback_level': 'intelligent_guess',
                'ultra_enhanced': True,
                'requires_confirmation': True
            }
        
        raise Exception("ImpossÃ­vel fazer suposiÃ§Ã£o vÃ¡lida") 
            negociacao_parcelamento_score += 0.2
        
        # Se tem datas, boost vencimento
        tem_data = any(e.type == 'datas' for e in entities)
        if tem_data:
            fatura_vencimento_score += 0.4
            pagamento_score += 0.2
        
        # ğŸ˜¤ BOOST BASEADO EM EMOÃ‡ÃƒO
        if emotion == 'frustrado':
            reclamacao_indevida_score += 0.4
            reclamacao_valor_score += 0.4
        elif emotion == 'urgente':
            fatura_solicitar_score += 0.3
            fatura_valor_score += 0.2
        elif emotion == 'confuso':
            duvida_score += 0.3
        
        # ğŸ¯ NORMALIZAR SCORES (max 1.0 para cada)
        intent_scores = {
            'fatura_solicitar': min(fatura_solicitar_score, 1.0),
            'fatura_valor': min(fatura_valor_score, 1.0),
            'fatura_vencimento': min(fatura_vencimento_score, 1.0),
            'negociacao_parcelamento': min(negociacao_parcelamento_score, 1.0),
            'negociacao_desconto': min(negociacao_desconto_score, 1.0),
            'pagamento_confirmacao': min(pagamento_score, 1.0),
            'reclamacao_cobranca_indevida': min(reclamacao_indevida_score, 1.0),
            'reclamacao_valor_incorreto': min(reclamacao_valor_score, 1.0),
            'saudacao': min(saudacao_score, 1.0),
            'despedida': min(despedida_score, 1.0),
            'confirmacao': min(confirmacao_score, 1.0),
            'negacao': min(negacao_score, 1.0),
            'duvida': min(duvida_score, 1.0)
        }
        
        # ğŸš¨ FALLBACK INTELIGENTE - Se nenhuma intenÃ§Ã£o forte foi detectada
        max_score = max(intent_scores.values()) if intent_scores.values() else 0
        if max_score < 0.3:
            # Cliente escreveu algo muito confuso - tentar inferir pela presenÃ§a de palavras-chave
            if any(palavra in text.lower() for palavra in ['conta', 'boleto', 'fatura', 'pagar', 'deve']):
                intent_scores['fatura_solicitar'] = 0.5  # Assumir que quer fatura
            elif any(palavra in text.lower() for palavra in ['quanto', 'valor', 'preÃ§o']):
                intent_scores['fatura_valor'] = 0.5  # Assumir que quer saber valor
            else:
                intent_scores['duvida'] = 0.5  # Cliente estÃ¡ confuso
        
        return intent_scores
    
    def _apply_contextual_boost(self, base_intents: Dict[str, float], context: Dict) -> Dict[str, float]:
        """Aplicar boost baseado no contexto conversacional"""
        boosted_intents = base_intents.copy()
        
        # Se Ãºltima mensagem foi sobre fatura, boost relacionados
        last_context = context.get('last_intent')
        if last_context and 'fatura' in last_context:
            boosted_intents['fatura_valor'] = boosted_intents.get('fatura_valor', 0) + 0.2
            boosted_intents['fatura_vencimento'] = boosted_intents.get('fatura_vencimento', 0) + 0.2
        
        # Se contexto de negociaÃ§Ã£o ativa
        if context.get('negotiation_active'):
            boosted_intents['negociacao_desconto'] = boosted_intents.get('negociacao_desconto', 0) + 0.3
            boosted_intents['confirmacao'] = boosted_intents.get('confirmacao', 0) + 0.2
        
        return boosted_intents
    
    def _detect_multiple_intents(self, text: str, entities: List[ExtractedEntity]) -> List[IntentType]:
        """Detectar mÃºltiplas intenÃ§Ãµes na mesma mensagem - MELHORADO"""
        intents = []
        
        # Detectores mais robustos de mÃºltiplas intenÃ§Ãµes
        
        # "fatura E desconto/parcelamento"
        if (re.search(r'(fatura|conta)', text, re.IGNORECASE) and 
            re.search(r'(tambÃ©m|e\s+(tambÃ©m)?).*(desconto|parcelar)', text, re.IGNORECASE)):
            intents.extend([IntentType.FATURA_SOLICITAR, IntentType.NEGOCIACAO_DESCONTO])
        
        # "fatura E parcelamento"  
        if (re.search(r'(fatura|conta)', text, re.IGNORECASE) and 
            re.search(r'(tambÃ©m|e\s+(tambÃ©m)?).*(parcelar|dividir)', text, re.IGNORECASE)):
            intents.extend([IntentType.FATURA_SOLICITAR, IntentType.NEGOCIACAO_PARCELAMENTO])
        
        # "paguei MAS ainda aparece"
        if (re.search(r'(paguei|quitei)', text, re.IGNORECASE) and 
            re.search(r'(mas|porÃ©m|ainda|continua|aparece)', text, re.IGNORECASE)):
            intents.extend([IntentType.PAGAMENTO_CONFIRMACAO, IntentType.RECLAMACAO_VALOR_INCORRETO])
        
        # "valor E vencimento"
        if (re.search(r'(quanto.*devo)', text, re.IGNORECASE) and 
            re.search(r'(quando.*vence|prazo)', text, re.IGNORECASE)):
            intents.extend([IntentType.FATURA_VALOR, IntentType.FATURA_VENCIMENTO])
        
        # Conectores brasileiros comuns
        conectores = [r'\s+e\s+', r'\s+tambÃ©m\s+', r'\s+alÃ©m\s+disso\s+', r'\s+mais\s+']
        for conector in conectores:
            if re.search(conector, text, re.IGNORECASE):
                # Se tem conector, analisar cada parte
                partes = re.split(conector, text, flags=re.IGNORECASE)
                if len(partes) >= 2:
                    # Analisar se cada parte tem intenÃ§Ã£o diferente
                    primeira_parte = partes[0].strip()
                    segunda_parte = partes[1].strip()
                    
                    # LÃ³gica simplificada para detectar intenÃ§Ãµes diferentes
                    if ('fatura' in primeira_parte.lower() and 
                        any(palavra in segunda_parte.lower() for palavra in ['desconto', 'parcelar', 'negociar'])):
                        intents.extend([IntentType.FATURA_SOLICITAR, IntentType.NEGOCIACAO_DESCONTO])
                        break
        
        return intents
    
    # MÃ©todos de normalizaÃ§Ã£o (implementaÃ§Ãµes simplificadas)
    def _normalize_currency(self, text: str) -> str:
        return re.sub(r'[^\d,]', '', text)
    
    def _normalize_date(self, text: str) -> str:
        return text.strip()
    
    def _normalize_protocol(self, text: str) -> str:
        return re.sub(r'[^\w\d]', '', text)
    
    def _normalize_document(self, text: str) -> str:
        return re.sub(r'[^\d]', '', text)
    
    def _super_normalize_text(self, text: str) -> str:
        """ğŸš€ NORMALIZAÃ‡ÃƒO ULTRA AVANÃ‡ADA - CORRIGE QUALQUER TEXTO MAL ESCRITO"""
        
        # 1. PRIMEIRA PASSADA - Limpeza bÃ¡sica
        text = text.lower().strip()
        
        # 2. REMOVER EMOJIS E CARACTERES ESPECIAIS (mas preservar pontuaÃ§Ã£o bÃ¡sica)
        text = re.sub(r'[^\w\s\.,!?\-Ã¡Ã Ã¢Ã£Ã©Ã¨ÃªÃ­Ã¬Ã®Ã³Ã²Ã´ÃµÃºÃ¹Ã»Ã§]', ' ', text)
        
        # 3. CORRIGIR ABREVIAÃ‡Ã•ES E ERROS COMUNS (do nosso dicionÃ¡rio)
        erro_patterns = self.brazilian_language_db.get('erro_patterns', {})
        for erro, correto in erro_patterns.items():
            # Usar word boundary para nÃ£o corrigir partes de palavras
            text = re.sub(rf'\b{re.escape(erro)}\b', correto, text, flags=re.IGNORECASE)
        
        # 4. CORREÃ‡Ã•ES ESPECÃFICAS DE PORTUGUÃŠS BRASILEIRO MAL ESCRITO
        corrections = {
            # Erros comuns de "quanto"
            r'\b(qnt|qnto|qto|cuanto)\b': 'quanto',
            # Erros comuns de "quando"  
            r'\b(qnd|qndo|quado|cuando)\b': 'quando',
            # Erros de "vocÃª"
            r'\bvc\b': 'vocÃª',
            # Erros de "nÃ£o"
            r'\b(nao|naum|Ã±|n)\b': 'nÃ£o',
            # Erros de "para"
            r'\b(pra|pr)\b': 'para',
            # Erros de "porque"
            r'\b(pq|pk|porq)\b': 'porque',
            # Erros de "tambÃ©m"
            r'\b(tb|tbm|tbn)\b': 'tambÃ©m',
            # Erros de "estÃ¡"
            r'\b(tah|ta|tÃ¡)\b': 'estÃ¡',
            # Erros de "estou"
            r'\b(to|tou)\b': 'estou',
            # Erros de "jÃ¡"
            r'\b(jah|ja)\b': 'jÃ¡',
            # Erros de "sÃ³"
            r'\b(soh|so)\b': 'sÃ³',
            # Erros de "Ã©"
            r'\b(eh|e)\b': 'Ã©',
            # Erros de "hoje"
            r'\bhj\b': 'hoje',
            # Erros de "amanhÃ£"
            r'\b(amanha|amÃ±)\b': 'amanhÃ£',
            # Erros de "cadÃª"
            r'\bkd\b': 'cadÃª',
            # Erros de "aqui"
            r'\b(aki|aq)\b': 'aqui',
            # Erros de "aÃ­"
            r'\b(ai|ae)\b': 'aÃ­',
            # Erros de "mesmo"
            r'\b(msm|mmo)\b': 'mesmo',
            # Erros de "beleza"
            r'\b(blz|bz)\b': 'beleza',
            # Erros de "valeu"
            r'\b(vlw|vl)\b': 'valeu',
            # Erros de "falou"
            r'\b(flw|fl)\b': 'falou',
            # Erros comuns de palavras de cobranÃ§a
            r'\b(fatur|ftur)\b': 'fatura',
            r'\b(bolto|bleto)\b': 'boleto',
            r'\b(cota|cnta)\b': 'conta',
            r'\b(cobransa|cobranca)\b': 'cobranÃ§a',
            r'\b(pagameto|pagamnto)\b': 'pagamento',
            r'\b(vencimeto|vencimto)\b': 'vencimento',
            r'\b(transferencia|trasferencia)\b': 'transferÃªncia',
            r'\b(debto|debito)\b': 'dÃ©bito'
        }
        
        for pattern, replacement in corrections.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        # 5. REMOVER PONTUAÃ‡ÃƒO EXCESSIVA mas preservar sentido
        text = re.sub(r'[!]{2,}', '!', text)
        text = re.sub(r'[?]{2,}', '?', text)
        text = re.sub(r'[.]{2,}', '...', text)
        
        # 6. NORMALIZAR ESPAÃ‡OS
        text = re.sub(r'\s+', ' ', text)
        
        # 7. CORREÃ‡Ã•ES CONTEXTUAIS ESPECÃFICAS PARA COBRANÃ‡A
        cobranca_corrections = {
            # "segunda via" mal escrito
            r'(segunda|2)\s*(v|vi|via)': 'segunda via',
            # "quanto devo" mal escrito  
            r'(quanto|qnto)\s*(devo|dvo|dveo)': 'quanto devo',
            # "jÃ¡ paguei" mal escrito
            r'(jÃ¡|jah|ja)\s*(paguei|pguei|pag)': 'jÃ¡ paguei',
            # "nÃ£o devo" mal escrito
            r'(nÃ£o|nao|naum)\s*(devo|dvo)': 'nÃ£o devo',
            # "minha conta" mal escrito
            r'(minha|miha)\s*(conta|cota)': 'minha conta'
        }
        
        for pattern, replacement in cobranca_corrections.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text.strip()
    
    def _get_conversation_context(self, phone: str) -> Dict:
        """Obter contexto da conversa"""
        return self.user_contexts.get(phone, {})
    
    def _update_conversation_context(self, phone: str, intent: ContextualIntent, text: str):
        """Atualizar contexto conversacional"""
        if phone not in self.user_contexts:
            self.user_contexts[phone] = {
                'messages': [],
                'last_intent': None,
                'negotiation_active': False,
                'client_profile': {},
                'conversation_flow': []
            }
        
        context = self.user_contexts[phone]
        context['messages'].append({
            'text': text,
            'intent': intent.intent.value,
            'confidence': intent.confidence,
            'timestamp': datetime.now(),
            'entities': [{'type': e.type, 'value': e.value} for e in intent.entities],
            'emotion': intent.emotional_state
        })
        
        context['last_intent'] = intent.intent.value
        
        # Detectar se negociaÃ§Ã£o estÃ¡ ativa
        if intent.intent in [IntentType.NEGOCIACAO_DESCONTO, IntentType.NEGOCIACAO_PARCELAMENTO]:
            context['negotiation_active'] = True
        
        # Manter apenas Ãºltimas 10 mensagens
        context['messages'] = context['messages'][-10:]
    
    async def _generate_contextual_response(
        self, phone: str, intent: ContextualIntent, entities: List[ExtractedEntity], context: Dict
    ) -> Dict[str, Any]:
        """ğŸš€ GERADOR DE RESPOSTAS ULTRA INTELIGENTE - PERFEITO PARA CLIENTES BURROS"""
        
        # ğŸ¯ RESPOSTAS BASEADAS NA INTENÃ‡ÃƒO COM CONTEXTO EMOCIONAL
        
        if intent.intent == IntentType.FATURA_SOLICITAR:
            if intent.emotional_state == 'urgente':
                response_text = "ğŸš¨ **URGENTE!** Entendi! Vou buscar sua fatura AGORA MESMO e te enviar em segundos!"
            elif intent.emotional_state == 'frustrado':
                response_text = "ğŸ˜” Percebo que vocÃª estÃ¡ chateado. Calma, vou resolver isso rapidinho! Enviando sua fatura jÃ¡..."
            elif intent.negation:
                response_text = "ğŸ¤” Vi que vocÃª disse 'nÃ£o' sobre algo. Me explica melhor o que vocÃª precisa da sua conta?"
            else:
                response_text = "ğŸ“„ **PERFEITO!** Vou pegar sua fatura para vocÃª. SÃ³ um minutinho..."
        
        elif intent.intent == IntentType.FATURA_VALOR:
            valor_entity = next((e for e in entities if e.type == 'valores_monetarios'), None)
            if valor_entity:
                response_text = f"ğŸ’° Vi que vocÃª mencionou **R$ {valor_entity.value}**. Vou confirmar se esse Ã© o valor correto da sua conta!"
            elif intent.emotional_state == 'urgente':
                response_text = "ğŸ’° **URGENTE!** Vou verificar AGORA quanto vocÃª deve exatamente!"
            else:
                response_text = "ğŸ’° Entendi! VocÃª quer saber **QUANTO DEVE**, certo? Vou verificar o valor da sua conta!"
        
        elif intent.intent == IntentType.FATURA_VENCIMENTO:
            data_entity = next((e for e in entities if e.type == 'datas'), None)
            if data_entity:
                response_text = f"â° Vi que vocÃª mencionou **{data_entity.value}**. Vou confirmar o vencimento da sua conta!"
            else:
                response_text = "â° Entendi! VocÃª quer saber **QUANDO VENCE** sua conta, nÃ©? Vou verificar a data!"
        
        elif intent.intent == IntentType.NEGOCIACAO_PARCELAMENTO:
            if intent.emotional_state == 'frustrado':
                response_text = "ğŸ¤ Entendo que estÃ¡ difÃ­cil pagar. **CALMA!** Vamos dar um jeito! Temos vÃ¡rias opÃ§Ãµes de parcelamento!"
            elif any(e.type == 'valores_monetarios' for e in entities):
                valor = next(e.value for e in entities if e.type == 'valores_monetarios')
                response_text = f"ğŸ¤ Perfeito! VocÃª quer parcelar **R$ {valor}**, nÃ©? Vamos encontrar a melhor condiÃ§Ã£o para vocÃª!"
            else:
                response_text = "ğŸ¤ **Ã“TIMO!** Quer parcelar? Vou ver as melhores condiÃ§Ãµes que temos disponÃ­veis!"
        
        elif intent.intent == IntentType.NEGOCIACAO_DESCONTO:
            if intent.emotional_state == 'frustrado':
                response_text = "ğŸ’¸ Entendo sua situaÃ§Ã£o! Vamos ver que **DESCONTO** posso conseguir para vocÃª!"
            else:
                response_text = "ğŸ’¸ Interessado em desconto? **PERFEITO!** Vou verificar as promoÃ§Ãµes disponÃ­veis!"
        
        elif intent.intent == IntentType.PAGAMENTO_CONFIRMACAO:
            if intent.temporal_context == 'passado':
                if intent.emotional_state == 'frustrado':
                    response_text = "âœ… Entendi! VocÃª **JÃ PAGOU** mas ainda estÃ¡ aparecendo, nÃ©? Vou verificar URGENTE o que aconteceu!"
            else:
                    response_text = "âœ… **BELEZA!** VocÃª jÃ¡ pagou! Vou confirmar aqui no sistema se o pagamento foi processado!"
            else:
                response_text = "ğŸ’³ Perfeito! Vou verificar o status do seu pagamento no sistema!"
        
        elif intent.intent == IntentType.RECLAMACAO_COBRANCA_INDEVIDA:
            if intent.emotional_state == 'frustrado':
                response_text = "ğŸ˜¡ **ENTENDO SUA REVOLTA!** CobranÃ§a indevida Ã© muito chato mesmo! Vou resolver isso AGORA!"
            else:
                response_text = "ğŸ” Entendi! VocÃª acha que essa cobranÃ§a estÃ¡ **ERRADA**, nÃ©? Vou analisar sua situaÃ§Ã£o!"
        
        elif intent.intent == IntentType.RECLAMACAO_VALOR_INCORRETO:
            response_text = "ğŸ” **NOSSA!** Valor incorreto Ã© sÃ©rio! Vou verificar sua conta e corrigir se estiver errado mesmo!"
        
        elif intent.intent == IntentType.SAUDACAO:
            horario = datetime.now().hour
            if horario < 12:
                response_text = "ğŸŒ… **BOM DIA!** Tudo beleza? Como posso te ajudar hoje?"
            elif horario < 18:
                response_text = "â˜€ï¸ **BOA TARDE!** E aÃ­, tudo certo? Em que posso ajudar?"
        else:
                response_text = "ğŸŒ™ **BOA NOITE!** Beleza? Como posso te ajudar?"
        
        elif intent.intent == IntentType.DESPEDIDA:
            response_text = "ğŸ‘‹ **VALEU!** Obrigado pelo contato! Qualquer coisa, me chama! ğŸ˜Š"
        
        elif intent.intent == IntentType.CONFIRMACAO:
            response_text = "âœ… **PERFEITO!** Entendi que vocÃª confirmou! Vou continuar com o processo!"
        
        elif intent.intent == IntentType.NEGACAO:
            response_text = "âŒ **BELEZA!** VocÃª disse que nÃ£o. Me explica melhor o que vocÃª precisa entÃ£o?"
        
        elif intent.intent == IntentType.DUVIDA:
            response_text = "ğŸ¤” **SEM PROBLEMAS!** Vou explicar melhor! O que especificamente vocÃª nÃ£o entendeu?"
        
        else:
            # Fallback inteligente baseado no que foi detectado
            if intent.confidence < 0.5:
                response_text = "ğŸ¤” **CALMA!** Acho que nÃ£o entendi direito. Pode me falar de novo de um jeito mais simples? Tipo: 'quero minha conta' ou 'quanto devo'?"
            else:
                response_text = "ğŸ¤– **ENTENDI ALGUMA COISA!** Mas me explica melhor o que vocÃª precisa. Fala de forma simples!"
        
        # ğŸ“‹ ADICIONAR INFORMAÃ‡Ã•ES SOBRE MÃšLTIPLAS INTENÃ‡Ã•ES
        if intent.multiple_intents and len(intent.multiple_intents) > 0:
            intents_text = []
            for multi_intent in intent.multiple_intents:
                if multi_intent == IntentType.FATURA_SOLICITAR:
                    intents_text.append("ver sua conta")
                elif multi_intent == IntentType.FATURA_VALOR:
                    intents_text.append("saber quanto deve")
                elif multi_intent == IntentType.NEGOCIACAO_PARCELAMENTO:
                    intents_text.append("parcelar")
                elif multi_intent == IntentType.NEGOCIACAO_DESCONTO:
                    intents_text.append("conseguir desconto")
                else:
                    intents_text.append(multi_intent.value.replace('_', ' '))
            
            if intents_text:
                response_text += f"\n\nğŸ“‹ **TAMBÃ‰M PERCEBI** que vocÃª quer: {' e '.join(intents_text)}. Vou ajudar com tudo!"
        
        # ğŸ”¥ ADICIONAR CALL TO ACTION BASEADO NA INTENÃ‡ÃƒO
        if intent.intent in [IntentType.FATURA_SOLICITAR, IntentType.FATURA_VALOR, IntentType.FATURA_VENCIMENTO]:
            response_text += "\n\nâš¡ **Aguarda aÃ­ que vou buscar suas informaÃ§Ãµes!**"
        elif intent.intent in [IntentType.NEGOCIACAO_PARCELAMENTO, IntentType.NEGOCIACAO_DESCONTO]:
            response_text += "\n\nğŸ¤ **Vou verificar as melhores condiÃ§Ãµes para vocÃª!**"
        elif intent.intent == IntentType.PAGAMENTO_CONFIRMACAO:
            response_text += "\n\nğŸ” **Verificando seu pagamento no sistema...**"
        
        return {
            'text': response_text,
            'intent': intent.intent.value,
            'confidence': intent.confidence,
            'entities_detected': len(entities),
            'emotional_state': intent.emotional_state,
            'multiple_intents': len(intent.multiple_intents),
            'context_enhanced': True,
            'response_type': 'ultra_contextual'
        }
    
    async def _generate_fallback_response(self, phone: str, text: str) -> Dict[str, Any]:
        """Resposta de fallback inteligente"""
        return {
            'text': "ğŸ¤” Percebi que vocÃª estÃ¡ tentando me dizer algo importante. Pode reformular para eu entender melhor?",
            'intent': 'clarification_needed',
            'confidence': 0.3,
            'fallback': True
        }
    
    def _load_contextual_responses(self) -> Dict:
        """Carregar respostas contextuais avanÃ§adas"""
        return {
            # Implementar depois conforme necessÃ¡rio
            'advanced_templates': {}
        } 
    
    # ================================
    # ğŸš€ SISTEMAS ULTRA AVANÃ‡ADOS - NÃVEL CHATGPT
    # ================================
    
    def _build_semantic_patterns(self) -> Dict[str, SemanticPattern]:
        """ğŸ§  CONSTRUIR PADRÃ•ES SEMÃ‚NTICOS ULTRA AVANÃ‡ADOS"""
        patterns = {}
        
        # PadrÃ£o semÃ¢ntico para FATURA
        patterns['fatura_semantic'] = SemanticPattern(
            pattern_id='fatura_semantic',
            semantic_vectors={
                'documento': 0.9, 'papel': 0.8, 'conta': 1.0, 'boleto': 1.0,
                'cobranÃ§a': 0.9, 'dÃ©bito': 0.8, 'pagamento': 0.7, 'valor': 0.6,
                'segunda_via': 1.0, 'cÃ³pia': 0.7, 'comprovante': 0.6
            },
            context_triggers=['preciso', 'quero', 'mandar', 'enviar', 'ver'],
            intent_weights={'fatura_solicitar': 1.0, 'fatura_valor': 0.3},
            emotional_indicators={'urgente': 0.3, 'neutro': 0.7},
            confidence_modifiers={'direto': 1.0, 'indireto': 0.7}
        )
        
        # PadrÃ£o semÃ¢ntico para VALOR/QUANTIDADE
        patterns['valor_semantic'] = SemanticPattern(
            pattern_id='valor_semantic',
            semantic_vectors={
                'quanto': 1.0, 'valor': 1.0, 'preÃ§o': 0.9, 'custo': 0.8,
                'dinheiro': 0.7, 'grana': 0.8, 'real': 0.6, 'centavo': 0.5,
                'total': 0.9, 'dever': 0.9, 'pagar': 0.8
            },
            context_triggers=['devo', 'pago', 'custa', 'vale'],
            intent_weights={'fatura_valor': 1.0, 'pagamento_confirmacao': 0.4},
            emotional_indicators={'frustrado': 0.2, 'neutro': 0.8},
            confidence_modifiers={'pergunta': 1.0, 'afirmacao': 0.6}
        )
        
        # PadrÃ£o semÃ¢ntico para TEMPO/VENCIMENTO
        patterns['tempo_semantic'] = SemanticPattern(
            pattern_id='tempo_semantic',
            semantic_vectors={
                'quando': 1.0, 'data': 0.9, 'dia': 0.8, 'prazo': 1.0,
                'vencimento': 1.0, 'vence': 1.0, 'atÃ©': 0.7, 'tempo': 0.8,
                'hoje': 0.6, 'amanhÃ£': 0.7, 'mÃªs': 0.6
            },
            context_triggers=['vence', 'termina', 'acaba', 'expira'],
            intent_weights={'fatura_vencimento': 1.0, 'pagamento_confirmacao': 0.3},
            emotional_indicators={'urgente': 0.5, 'neutro': 0.5},
            confidence_modifiers={'futuro': 1.0, 'passado': 0.4}
        )
        
        # PadrÃ£o semÃ¢ntico para NEGOCIAÃ‡ÃƒO
        patterns['negociacao_semantic'] = SemanticPattern(
            pattern_id='negociacao_semantic',
            semantic_vectors={
                'parcelar': 1.0, 'dividir': 0.9, 'acordo': 0.9, 'negociar': 1.0,
                'desconto': 1.0, 'abatimento': 0.8, 'facilitar': 0.7, 'ajuda': 0.6,
                'dificuldade': 0.8, 'problema': 0.7, 'apertado': 0.8, 'quebrar_galho': 0.9
            },
            context_triggers=['nÃ£o_consigo', 'difÃ­cil', 'sem_dinheiro', 'ajudar'],
            intent_weights={'negociacao_parcelamento': 0.7, 'negociacao_desconto': 0.3},
            emotional_indicators={'frustrado': 0.6, 'urgente': 0.4},
            confidence_modifiers={'pedido': 1.0, 'sugestao': 0.8}
        )
        
        return patterns
    
    def _build_semantic_vectors(self) -> Dict[str, Dict[str, float]]:
        """ğŸ”¬ CONSTRUIR VETORES SEMÃ‚NTICOS BRASILEIROS ULTRA AVANÃ‡ADOS"""
        return {
            # Vetores semÃ¢nticos para palavras de cobranÃ§a
            'fatura': {
                'conta': 0.95, 'boleto': 0.90, 'cobranÃ§a': 0.85, 'dÃ©bito': 0.80,
                'documento': 0.75, 'papel': 0.70, 'segunda_via': 0.95, 'cÃ³pia': 0.60
            },
            'pagar': {
                'quitar': 0.90, 'saldar': 0.85, 'liquidar': 0.80, 'acertar': 0.75,
                'resolver': 0.70, 'transferir': 0.65, 'depositar': 0.60
            },
            'quanto': {
                'valor': 0.95, 'preÃ§o': 0.90, 'custo': 0.85, 'total': 0.80,
                'dinheiro': 0.75, 'grana': 0.80, 'real': 0.70
            },
            'quando': {
                'data': 0.90, 'dia': 0.85, 'prazo': 0.95, 'vencimento': 0.95,
                'tempo': 0.80, 'atÃ©': 0.75, 'hora': 0.70
            },
            'problema': {
                'dificuldade': 0.90, 'complicaÃ§Ã£o': 0.85, 'erro': 0.80,
                'confusÃ£o': 0.75, 'encrenca': 0.85, 'pepino': 0.80
            }
        }
    
    def _build_intent_similarity_matrix(self) -> Dict[str, Dict[str, float]]:
        """ğŸ¯ MATRIZ DE SIMILARIDADE ENTRE INTENÃ‡Ã•ES"""
        return {
            'fatura_solicitar': {
                'fatura_valor': 0.7, 'fatura_vencimento': 0.6, 'pagamento_confirmacao': 0.4,
                'negociacao_parcelamento': 0.3, 'informacao_conta': 0.8
            },
            'fatura_valor': {
                'fatura_solicitar': 0.7, 'fatura_vencimento': 0.5, 'pagamento_confirmacao': 0.6,
                'negociacao_parcelamento': 0.7, 'negociacao_desconto': 0.5
            },
            'negociacao_parcelamento': {
                'negociacao_desconto': 0.8, 'pagamento_dificuldade': 0.9, 'fatura_valor': 0.6
            },
            'pagamento_confirmacao': {
                'reclamacao_valor_incorreto': 0.5, 'fatura_valor': 0.4, 'fatura_solicitar': 0.3
            }
        }
    
    def _build_relationship_graph(self) -> Dict[str, List[str]]:
        """ğŸ•¸ï¸ GRAFO DE RELACIONAMENTOS CONTEXTUAIS"""
        return {
            'financial_entities': ['valor', 'dinheiro', 'real', 'centavo', 'pagar', 'dever'],
            'temporal_entities': ['quando', 'dia', 'data', 'prazo', 'vencimento', 'atÃ©'],
            'document_entities': ['conta', 'boleto', 'fatura', 'papel', 'documento', 'cÃ³pia'],
            'negotiation_entities': ['parcelar', 'dividir', 'acordo', 'desconto', 'facilitar'],
            'emotional_entities': ['problema', 'dificuldade', 'urgente', 'chateado', 'nervoso'],
            'action_entities': ['quero', 'preciso', 'gostaria', 'mandar', 'enviar', 'ver']
        }
    
    def _load_discourse_analyzers(self) -> Dict[str, Any]:
        """ğŸ’¬ ANALISADORES DE DISCURSO ULTRA AVANÃ‡ADOS"""
        return {
            'discourse_markers': {
                'addition': ['tambÃ©m', 'alÃ©m disso', 'e', 'mais', 'ainda'],
                'contrast': ['mas', 'porÃ©m', 'entretanto', 'contudo', 'no entanto'],
                'cause': ['porque', 'pois', 'jÃ¡ que', 'visto que', 'uma vez que'],
                'conclusion': ['entÃ£o', 'portanto', 'assim', 'logo', 'por isso'],
                'sequence': ['primeiro', 'depois', 'em seguida', 'finalmente', 'por Ãºltimo'],
                'emphasis': ['realmente', 'muito', 'bastante', 'extremamente', 'totalmente']
            },
            'pragmatic_markers': {
                'politeness': ['por favor', 'obrigado', 'desculpa', 'com licenÃ§a'],
                'urgency': ['urgente', 'rÃ¡pido', 'agora', 'imediatamente', 'jÃ¡'],
                'uncertainty': ['acho', 'talvez', 'pode ser', 'nÃ£o tenho certeza'],
                'emphasis': ['realmente', 'certamente', 'definitivamente', 'com certeza']
            }
        }
    
    def _build_pragmatic_engine(self) -> Dict[str, Any]:
        """ğŸ§  ENGINE DE INFERÃŠNCIA PRAGMÃTICA ULTRA AVANÃ‡ADA"""
        return {
            'implicature_rules': {
                # Se diz "jÃ¡ paguei MAS ainda aparece" = reclama valor incorreto
                'payment_but_still_charged': {
                    'pattern': r'(jÃ¡.*pagu|quitei|paguei).*(mas|porÃ©m|ainda|continua)',
                    'inference': 'reclamacao_valor_incorreto',
                    'confidence': 0.9
                },
                # Se pergunta valor E prazo = quer informaÃ§Ãµes completas
                'value_and_deadline': {
                    'pattern': r'(quanto.*devo).*(quando.*vence|prazo)',
                    'inference': 'multiple_intents',
                    'confidence': 0.8
                },
                # Se diz que nÃ£o consegue pagar = quer negociar
                'cannot_pay': {
                    'pattern': r'nÃ£o.*(consigo|posso).*(pagar|quitar)',
                    'inference': 'negociacao_parcelamento',
                    'confidence': 0.85
                }
            },
            'contextual_inference': {
                # InferÃªncias baseadas no contexto da conversa
                'follow_up_questions': {
                    'after_invoice_request': ['fatura_valor', 'fatura_vencimento'],
                    'after_negotiation': ['confirmacao', 'negacao', 'duvida'],
                    'after_payment_info': ['pagamento_confirmacao']
                }
            }
        }
    
    def _build_coherence_analyzer(self) -> Dict[str, Any]:
        """ğŸ”— ANALISADOR DE COERÃŠNCIA CONTEXTUAL ULTRA AVANÃ‡ADO"""
        return {
            'coherence_rules': {
                'topic_continuity': {
                    'same_topic': 1.0,      # Mesma intenÃ§Ã£o que anterior
                    'related_topic': 0.8,   # IntenÃ§Ã£o relacionada
                    'topic_shift': 0.4,     # MudanÃ§a de assunto
                    'random_topic': 0.1     # Assunto totalmente aleatÃ³rio
                },
                'temporal_coherence': {
                    'logical_sequence': 1.0,    # SequÃªncia lÃ³gica
                    'acceptable_jump': 0.7,     # Salto aceitÃ¡vel
                    'confusing_sequence': 0.3   # SequÃªncia confusa
                }
            },
            'context_memory_window': 5,  # Quantas mensagens anteriores considerar
            'coherence_threshold': 0.6   # Limite mÃ­nimo de coerÃªncia
        }
    
    def _build_multi_layer_processors(self) -> List[Dict[str, Any]]:
        """ğŸ›ï¸ PROCESSADORES MULTI-CAMADAS ULTRA AVANÃ‡ADOS"""
        return [
            {
                'layer': 'lexical',
                'processor': 'word_level_analysis',
                'weight': 0.2,
                'functions': ['tokenization', 'pos_tagging', 'lemmatization']
            },
            {
                'layer': 'syntactic', 
                'processor': 'phrase_level_analysis',
                'weight': 0.3,
                'functions': ['phrase_detection', 'dependency_parsing']
            },
            {
                'layer': 'semantic',
                'processor': 'meaning_level_analysis', 
                'weight': 0.3,
                'functions': ['semantic_similarity', 'concept_mapping']
            },
            {
                'layer': 'pragmatic',
                'processor': 'context_level_analysis',
                'weight': 0.2,
                'functions': ['pragmatic_inference', 'discourse_analysis']
            }
        ]
    
    def _build_fallback_system(self) -> Dict[str, Any]:
        """ğŸ›¡ï¸ SISTEMA DE FALLBACK INTELIGENTE MULTI-CAMADAS"""
        return {
            'fallback_levels': [
                {
                    'level': 1,
                    'name': 'semantic_similarity',
                    'method': 'find_closest_semantic_match',
                    'threshold': 0.6
                },
                {
                    'level': 2, 
                    'name': 'keyword_extraction',
                    'method': 'extract_key_concepts',
                    'threshold': 0.4
                },
                {
                    'level': 3,
                    'name': 'pattern_matching',
                    'method': 'fuzzy_pattern_match', 
                    'threshold': 0.3
                },
                {
                    'level': 4,
                    'name': 'conversational_context',
                    'method': 'infer_from_conversation',
                    'threshold': 0.2
                },
                {
                    'level': 5,
                    'name': 'intelligent_guess',
                    'method': 'make_educated_guess',
                    'threshold': 0.1
                }
            ]
        }
    
    def _build_dynamic_generator(self) -> Dict[str, Any]:
        """ğŸ­ GERADOR DINÃ‚MICO DE RESPOSTAS ULTRA INTELIGENTE"""
        return {
            'response_templates': {
                'high_confidence': "âœ… **{emotion_marker}** {action_confirmation} {specifics}",
                'medium_confidence': "ğŸ¤” **{understanding}** {clarification_request}",
                'low_confidence': "â“ **{confusion_acknowledgment}** {help_request}",
                'contextual': "ğŸ¯ **{context_reference}** {personalized_response}"
            },
            'emotion_markers': {
                'urgente': ['URGENTE!', 'RAPIDINHO!', 'AGORA MESMO!'],
                'frustrado': ['CALMA!', 'ENTENDO!', 'VAMOS RESOLVER!'],
                'neutro': ['PERFEITO!', 'BELEZA!', 'CERTO!'],
                'satisfeito': ['Ã“TIMO!', 'EXCELENTE!', 'SHOW!']
            },
            'personalization_factors': [
                'conversation_history', 'emotional_state', 'communication_style',
                'previous_intents', 'response_patterns', 'user_preferences'
            ]
        }
    
    # ================================
    # ğŸš€ MÃ‰TODOS ULTRA MEGA AVANÃ‡ADOS - NÃVEL CHATGPT GIGANTEMENTE FODA
    # ================================
    
    def _get_or_create_conversation_memory(self, phone: str) -> ConversationMemory:
        """ğŸ§  OBTER OU CRIAR MEMÃ“RIA ULTRA AVANÃ‡ADA"""
        if phone not in self.conversation_memories:
            self.conversation_memories[phone] = ConversationMemory()
        return self.conversation_memories[phone]
    
    def _ultra_advanced_normalize_text(self, text: str) -> str:
        """ğŸš€ NORMALIZAÃ‡ÃƒO ULTRA MEGA AVANÃ‡ADA"""
        # Usar o mÃ©todo existente mas com melhorias
        normalized = self._super_normalize_text(text)
        
        # Adicionar anÃ¡lises extras ultra avanÃ§adas
        normalized = self._apply_phonetic_corrections(normalized)
        normalized = self._fix_cognitive_errors(normalized)
        normalized = self._standardize_brazilian_expressions(normalized)
        
        return normalized
    
    def _apply_phonetic_corrections(self, text: str) -> str:
        """ğŸ”Š CORREÃ‡Ã•ES FONÃ‰TICAS ULTRA AVANÃ‡ADAS"""
        phonetic_corrections = {
            # CorreÃ§Ãµes baseadas em como as pessoas falam
            r'\b(di)\b': 'de',  # "di manhÃ£" -> "de manhÃ£"
            r'\b(nu)\b': 'no',  # "nu banco" -> "no banco"
            r'\b(du)\b': 'do',  # "du cliente" -> "do cliente"
            r'\b(ma)\b': 'mas', # "ma nÃ£o" -> "mas nÃ£o"
            r'\b(qui)\b': 'que', # "qui dia" -> "que dia"
            r'\b(cumÃ©)\b': 'como Ã©', # "cumÃ© que" -> "como Ã© que"
            r'\b(ocÃª)\b': 'vocÃª',    # "ocÃª tem" -> "vocÃª tem"
            r'\b(seje)\b': 'seja',   # "seje o que" -> "seja o que"
        }
        
        for pattern, replacement in phonetic_corrections.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text
    
    def _fix_cognitive_errors(self, text: str) -> str:
        """ğŸ§  CORRIGIR ERROS COGNITIVOS E DE RACIOCÃNIO"""
        cognitive_fixes = {
            # Erros de lÃ³gica temporal
            r'(ontem.*amanha|amanha.*ontem)': 'ontem ou amanhÃ£',
            # ContradiÃ§Ãµes Ã³bvias
            r'(nÃ£o.*mas.*sim|sim.*mas.*nÃ£o)': 'talvez',
            # ConfusÃµes de pessoa
            r'(vocÃª.*eu.*pagar|eu.*vocÃª.*pagar)': 'preciso pagar',
        }
        
        for pattern, replacement in cognitive_fixes.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text
    
    def _standardize_brazilian_expressions(self, text: str) -> str:
        """ğŸ‡§ğŸ‡· PADRONIZAR EXPRESSÃ•ES TIPICAMENTE BRASILEIRAS"""
        expressions = {
            r'(tÃ¡.*ligado|sacou|entendeu)': 'entende',
            r'(massa|show|da.*hora)': 'bom',
            r'(trampo|labuta)': 'trabalho',
            r'(grana|din.*din|money)': 'dinheiro',
            r'(mina|mano|brother)': 'pessoa',
            r'(rolÃª|role)': 'situaÃ§Ã£o',
        }
        
        for pattern, replacement in expressions.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text
    
    async def _perform_multi_layer_analysis(self, text: str) -> Dict[str, Any]:
        """ğŸ›ï¸ ANÃLISE MULTI-CAMADAS ULTRA PROFUNDA"""
        analysis = {
            'lexical': self._analyze_lexical_layer(text),
            'syntactic': self._analyze_syntactic_layer(text),
            'semantic': self._analyze_semantic_layer(text),
            'pragmatic': self._analyze_pragmatic_layer(text)
        }
        
        # Calcular score agregado
        analysis['overall_complexity'] = sum(
            layer['complexity_score'] * processor['weight'] 
            for layer, processor in zip(analysis.values(), self.multi_layer_processors)
        )
        
        return analysis
    
    def _analyze_lexical_layer(self, text: str) -> Dict[str, Any]:
        """ğŸ“ ANÃLISE LEXICAL ULTRA PROFUNDA"""
        words = text.split()
        
        return {
            'word_count': len(words),
            'avg_word_length': sum(len(w) for w in words) / len(words) if words else 0,
            'complexity_score': min(len(words) * 0.1, 1.0),
            'rare_words': [w for w in words if len(w) > 8],
            'simple_words': [w for w in words if len(w) <= 4]
        }
    
    def _analyze_syntactic_layer(self, text: str) -> Dict[str, Any]:
        """ğŸ”— ANÃLISE SINTÃTICA ULTRA PROFUNDA"""
        # Detectar estruturas sintÃ¡ticas
        has_questions = bool(re.search(r'\?', text))
        has_subordinate = bool(re.search(r'\b(que|se|quando|onde|como)\b', text))
        has_coordination = bool(re.search(r'\b(e|mas|ou|porÃ©m)\b', text))
        
        complexity = 0.3
        if has_questions: complexity += 0.2
        if has_subordinate: complexity += 0.3
        if has_coordination: complexity += 0.2
        
        return {
            'has_questions': has_questions,
            'has_subordinate_clauses': has_subordinate,
            'has_coordination': has_coordination,
            'complexity_score': min(complexity, 1.0)
        }
    
    def _analyze_semantic_layer(self, text: str) -> Dict[str, Any]:
        """ğŸ§  ANÃLISE SEMÃ‚NTICA ULTRA PROFUNDA"""
        semantic_clusters = []
        cluster_scores = {}
        
        # Analisar proximidade semÃ¢ntica com nossos clusters
        for cluster_name, words in self.contextual_relationship_graph.items():
            score = 0
            for word in words:
                if word in text.lower():
                    score += 1
            
            if score > 0:
                semantic_clusters.append(cluster_name)
                cluster_scores[cluster_name] = score / len(words)
        
        return {
            'semantic_clusters': semantic_clusters,
            'cluster_scores': cluster_scores,
            'complexity_score': min(len(semantic_clusters) * 0.2, 1.0),
            'semantic_density': sum(cluster_scores.values()) / max(len(cluster_scores), 1)
        }
    
    def _analyze_pragmatic_layer(self, text: str) -> Dict[str, Any]:
        """ğŸ’­ ANÃLISE PRAGMÃTICA ULTRA PROFUNDA"""
        pragmatic_elements = {}
        
        # Detectar elementos pragmÃ¡ticos
        for marker_type, markers in self.discourse_analyzers['pragmatic_markers'].items():
            found_markers = [m for m in markers if m in text.lower()]
            if found_markers:
                pragmatic_elements[marker_type] = found_markers
        
        return {
            'pragmatic_elements': pragmatic_elements,
            'complexity_score': min(len(pragmatic_elements) * 0.25, 1.0),
            'pragmatic_richness': len(pragmatic_elements)
        }
    
    async def _perform_semantic_analysis(self, text: str, memory: ConversationMemory) -> Dict[str, Any]:
        """ğŸ”¬ ANÃLISE SEMÃ‚NTICA MEGA ULTRA AVANÃ‡ADA"""
        semantic_analysis = {}
        
        # Calcular similaridade semÃ¢ntica com padrÃµes conhecidos
        for pattern_id, pattern in self.semantic_patterns.items():
            similarity = self._calculate_semantic_similarity(text, pattern)
            semantic_analysis[pattern_id] = similarity
        
        # AnÃ¡lise de vetores semÃ¢nticos
        vector_analysis = self._analyze_semantic_vectors(text)
        
        return {
            'pattern_similarities': semantic_analysis,
            'vector_analysis': vector_analysis,
            'best_match': max(semantic_analysis.items(), key=lambda x: x[1]) if semantic_analysis else None,
            'semantic_confidence': max(semantic_analysis.values()) if semantic_analysis else 0.0
        }
    
    def _calculate_semantic_similarity(self, text: str, pattern: SemanticPattern) -> float:
        """ğŸ“ CALCULAR SIMILARIDADE SEMÃ‚NTICA ULTRA PRECISA"""
        similarity_score = 0.0
        total_weight = 0.0
        
        # Analisar vetores semÃ¢nticos
        for concept, weight in pattern.semantic_vectors.items():
            if concept in text.lower():
                similarity_score += weight
            total_weight += weight
        
        # Normalizar score
        if total_weight > 0:
            similarity_score = similarity_score / total_weight
        
        # Boost por triggers contextuais
        for trigger in pattern.context_triggers:
            if trigger in text.lower():
                similarity_score += 0.1
        
        return min(similarity_score, 1.0)
    
    def _analyze_semantic_vectors(self, text: str) -> Dict[str, float]:
        """ğŸ§® ANÃLISE DE VETORES SEMÃ‚NTICOS"""
        vector_scores = {}
        
        for main_concept, related_concepts in self.brazilian_semantic_vectors.items():
            if main_concept in text.lower():
                vector_scores[main_concept] = 1.0
                
                # Adicionar conceitos relacionados
                for related, similarity in related_concepts.items():
                    if related in text.lower():
                        vector_scores[related] = similarity
        
        return vector_scores
    
    async def _perform_pragmatic_analysis(self, text: str, memory: ConversationMemory) -> Dict[str, Any]:
        """ğŸ­ ANÃLISE PRAGMÃTICA MEGA ULTRA AVANÃ‡ADA"""
        pragmatic_inferences = {}
        
        # Aplicar regras de implicatura
        for rule_name, rule in self.pragmatic_inference_engine['implicature_rules'].items():
            if re.search(rule['pattern'], text, re.IGNORECASE):
                pragmatic_inferences[rule_name] = {
                    'inference': rule['inference'],
                    'confidence': rule['confidence']
                }
        
        # AnÃ¡lise contextual baseada na conversa anterior
        contextual_inferences = self._analyze_conversational_context(text, memory)
        
        return {
            'implicatures': pragmatic_inferences,
            'contextual_inferences': contextual_inferences,
            'pragmatic_confidence': max(
                [inf['confidence'] for inf in pragmatic_inferences.values()] + [0.0]
            )
        }
    
    def _analyze_conversational_context(self, text: str, memory: ConversationMemory) -> Dict[str, Any]:
        """ğŸ’¬ ANÃLISE DE CONTEXTO CONVERSACIONAL ULTRA PROFUNDA"""
        inferences = {}
        
        # Analisar padrÃ£o baseado na Ãºltima intenÃ§Ã£o
        if memory.intent_history:
            last_intent, confidence, timestamp = memory.intent_history[-1]
            
            # Inferir follow-ups baseados na intenÃ§Ã£o anterior
            follow_ups = self.pragmatic_inference_engine['contextual_inference']['follow_up_questions']
            if last_intent in follow_ups:
                for possible_intent in follow_ups[last_intent]:
                    inferences[f'follow_up_{possible_intent}'] = confidence * 0.7
        
        return inferences
    
    async def _extract_ultra_advanced_entities(self, text: str, semantic_analysis: Dict[str, Any]) -> List[ExtractedEntity]:
        """ğŸ¯ EXTRAÃ‡ÃƒO ULTRA AVANÃ‡ADA DE ENTIDADES COM CONTEXTO SEMÃ‚NTICO"""
        entities = []
        
        # Usar mÃ©todo existente como base
        base_entities = self._extract_all_entities(text)
        
        # Enriquecer com anÃ¡lise semÃ¢ntica
        for entity in base_entities:
            # Calcular peso semÃ¢ntico
            semantic_weight = 1.0
            if semantic_analysis.get('vector_analysis'):
                for concept, score in semantic_analysis['vector_analysis'].items():
                    if concept in entity.value.lower():
                        semantic_weight = max(semantic_weight, score)
            
            # Adicionar alternativas baseadas em similaridade
            alternatives = self._find_entity_alternatives(entity, semantic_analysis)
            
            # Criar entidade enriquecida
            ultra_entity = ExtractedEntity(
                type=entity.type,
                value=entity.value,
                confidence=entity.confidence,
                context=entity.context,
                semantic_weight=semantic_weight,
                alternatives=alternatives,
                relationships=self._find_entity_relationships(entity, text)
            )
            
            entities.append(ultra_entity)
        
        return entities
    
    def _find_entity_alternatives(self, entity: ExtractedEntity, semantic_analysis: Dict[str, Any]) -> List[str]:
        """ğŸ” ENCONTRAR ALTERNATIVAS SEMÃ‚NTICAS PARA ENTIDADES"""
        alternatives = []
        
        if entity.type == 'valores_monetarios':
            alternatives = ['valor', 'quantia', 'dinheiro', 'preÃ§o', 'custo']
        elif entity.type == 'datas':
            alternatives = ['prazo', 'vencimento', 'data', 'dia', 'quando']
        
        return alternatives
    
    def _find_entity_relationships(self, entity: ExtractedEntity, text: str) -> Dict[str, float]:
        """ğŸ•¸ï¸ ENCONTRAR RELACIONAMENTOS ENTRE ENTIDADES"""
        relationships = {}
        
        # Analisar proximidade com outras palavras-chave
        for cluster_name, words in self.contextual_relationship_graph.items():
            for word in words:
                if word in text.lower() and word != entity.value.lower():
                    relationships[word] = 0.8  # Score de relacionamento
        
        return relationships
    
    async def _analyze_ultra_emotion(self, text: str, memory: ConversationMemory) -> str:
        """ğŸ˜Š ANÃLISE EMOCIONAL ULTRA AVANÃ‡ADA COM MEMÃ“RIA"""
        # Usar anÃ¡lise existente como base
        base_emotion = self._analyze_emotion(text)
        
        # Enriquecer com contexto de memÃ³ria emocional
        if memory.emotional_journey:
            # Considerar padrÃ£o emocional histÃ³rico
            recent_emotions = [emotion for emotion, score, timestamp in memory.emotional_journey[-3:]]
            
            # Se hÃ¡ padrÃ£o de frustraÃ§Ã£o crescente
            if recent_emotions.count('frustrado') >= 2:
                if base_emotion in ['neutro', 'confuso']:
                    base_emotion = 'frustrado'  # Inferir frustraÃ§Ã£o continuada
        
        # Detectar escalation emocional
        emotional_escalation = self._detect_emotional_escalation(text)
        if emotional_escalation:
            if base_emotion == 'frustrado':
                base_emotion = 'muito_frustrado'  # Nova categoria
            elif base_emotion == 'urgente':
                base_emotion = 'extremamente_urgente'  # Nova categoria
        
        return base_emotion
    
    def _detect_emotional_escalation(self, text: str) -> bool:
        """ğŸ“ˆ DETECTAR ESCALATION EMOCIONAL"""
        escalation_markers = [
            r'(muito|extremamente|super|ultra).*(chateado|irritado)',
            r'(nÃ£o.*aguentar|nÃ£o.*suportar)',
            r'(absurdo|ridÃ­culo|inaceitÃ¡vel)',
            r'[!]{3,}',  # MÃºltiplas exclamaÃ§Ãµes
            r'[?!]{2,}',  # Mistura de ? e !
        ]
        
        return any(re.search(pattern, text, re.IGNORECASE) for pattern in escalation_markers)
    
    async def _analyze_ultra_temporal_context(self, text: str, memory: ConversationMemory) -> str:
        """â° ANÃLISE TEMPORAL ULTRA AVANÃ‡ADA"""
        base_temporal = self._analyze_temporal_context(text)
        
        # Enriquecer com anÃ¡lise de urgÃªncia temporal
        urgency_indicators = {
            'imediato': ['agora', 'jÃ¡', 'imediatamente', 'urgente'],
            'hoje': ['hoje', 'hj', 'ainda hoje'],
            'breve': ['logo', 'em breve', 'rapidinho'],
            'futuro_proximo': ['amanhÃ£', 'essa semana', 'uns dias'],
            'futuro_distante': ['mÃªs que vem', 'ano que vem', 'mais tarde']
        }
        
        for urgency_level, indicators in urgency_indicators.items():
            if any(indicator in text.lower() for indicator in indicators):
                return f"{base_temporal}_{urgency_level}"
        
        return base_temporal
    
    async def _analyze_ultra_negation(self, text: str) -> Dict[str, Any]:
        """âŒ ANÃLISE ULTRA AVANÃ‡ADA DE NEGAÃ‡ÃƒO"""
        has_basic_negation = self._detect_negation(text)
        
        # AnÃ¡lise mais sofisticada de tipos de negaÃ§Ã£o
        negation_types = {
            'absolute': r'\b(nunca|jamais|de jeito nenhum)\b',
            'partial': r'\b(nÃ£o muito|meio que nÃ£o|acho que nÃ£o)\b',
            'conditional': r'\b(nÃ£o se|sÃ³ nÃ£o|a nÃ£o ser)\b',
            'emphatic': r'\b(de forma alguma|nem pensar|que nada)\b'
        }
        
        detected_types = []
        for neg_type, pattern in negation_types.items():
            if re.search(pattern, text, re.IGNORECASE):
                detected_types.append(neg_type)
        
        return {
            'has_negation': has_basic_negation,
            'negation_types': detected_types,
            'negation_strength': len(detected_types) / len(negation_types)
        }
    
    async def _analyze_ultra_contextual_intent(
        self, text: str, entities: List[ExtractedEntity], emotion: str, 
        temporal: str, negation: Dict, memory: ConversationMemory, 
        semantic_analysis: Dict, pragmatic_analysis: Dict
    ) -> ContextualIntent:
        """ğŸ§  ANÃLISE ULTRA MEGA AVANÃ‡ADA DE INTENÃ‡ÃƒO CONTEXTUAL"""
        
        # Usar anÃ¡lise base existente
        base_intent_analysis = self._analyze_contextual_intent(
            text, entities, emotion, temporal, negation.get('has_negation', False), memory
        )
        
        # ENRIQUECER COM ANÃLISES ULTRA AVANÃ‡ADAS
        
        # 1. Boost semÃ¢ntico baseado na melhor correspondÃªncia
        if semantic_analysis.get('best_match'):
            pattern_id, similarity_score = semantic_analysis['best_match']
            if similarity_score > 0.7:
                # Aplicar boost baseado no padrÃ£o semÃ¢ntico
                if 'fatura' in pattern_id:
                    base_intent_analysis.confidence += 0.2
                elif 'valor' in pattern_id:
                    base_intent_analysis.confidence += 0.15
        
        # 2. Boost pragmÃ¡tico baseado em implicaturas
        pragmatic_confidence = pragmatic_analysis.get('pragmatic_confidence', 0)
        base_intent_analysis.confidence += pragmatic_confidence * 0.1
        
        # 3. Calcular similaridade semÃ¢ntica com intenÃ§Ãµes conhecidas
        semantic_similarity = self._calculate_intent_semantic_similarity(
            base_intent_analysis.intent, semantic_analysis
        )
        
        # 4. Analisar alternativas de intenÃ§Ã£o
        alternative_intents = self._calculate_alternative_intents(
            text, semantic_analysis, pragmatic_analysis
        )
        
        # 5. Detectar clusters semÃ¢nticos
        semantic_clusters = semantic_analysis.get('pattern_similarities', {}).keys()
        
        # 6. Analisar marcadores de discurso
        discourse_markers = self._extract_discourse_markers(text)
        
        # 7. InferÃªncia pragmÃ¡tica ultra avanÃ§ada
        pragmatic_inference = self._calculate_pragmatic_inference(
            base_intent_analysis, pragmatic_analysis, memory
        )
        
        # Criar intenÃ§Ã£o contextual ultra enriquecida
        ultra_intent = ContextualIntent(
            intent=base_intent_analysis.intent,
            confidence=min(base_intent_analysis.confidence, 1.0),
            entities=entities,
            temporal_context=temporal,
            emotional_state=emotion,
            negation=negation.get('has_negation', False),
            multiple_intents=base_intent_analysis.multiple_intents,
            
            # CAMPOS ULTRA AVANÃ‡ADOS
            semantic_similarity=semantic_similarity,
            contextual_coherence=0.0,  # SerÃ¡ calculado depois
            linguistic_complexity=semantic_analysis.get('semantic_confidence', 0),
            intent_certainty=0.0,  # SerÃ¡ calculado depois
            alternative_intents=alternative_intents,
            semantic_clusters=list(semantic_clusters),
            discourse_markers=discourse_markers,
            pragmatic_inference=pragmatic_inference
        )
        
        return ultra_intent
    
    def _calculate_intent_semantic_similarity(self, intent: IntentType, semantic_analysis: Dict) -> float:
        """ğŸ“ CALCULAR SIMILARIDADE SEMÃ‚NTICA DA INTENÃ‡ÃƒO"""
        intent_key = intent.value
        similarity_matrix = self.intent_similarity_matrix
        
        if intent_key in similarity_matrix:
            # Calcular mÃ©dia das similaridades com outras intenÃ§Ãµes detectadas
            similarities = []
            for related_intent, similarity in similarity_matrix[intent_key].items():
                if any(related_intent in cluster for cluster in semantic_analysis.get('pattern_similarities', {})):
                    similarities.append(similarity)
            
            return sum(similarities) / len(similarities) if similarities else 0.5
        
        return 0.5  # Default
    
    def _calculate_alternative_intents(self, text: str, semantic_analysis: Dict, pragmatic_analysis: Dict) -> List[Tuple[IntentType, float]]:
        """ğŸ¯ CALCULAR INTENÃ‡Ã•ES ALTERNATIVAS"""
        alternatives = []
        
        # Baseado em anÃ¡lise semÃ¢ntica
        for pattern_id, similarity in semantic_analysis.get('pattern_similarities', {}).items():
            if similarity > 0.5:
                if 'fatura' in pattern_id:
                    alternatives.append((IntentType.FATURA_SOLICITAR, similarity))
                elif 'valor' in pattern_id:
                    alternatives.append((IntentType.FATURA_VALOR, similarity))
                elif 'negociacao' in pattern_id:
                    alternatives.append((IntentType.NEGOCIACAO_PARCELAMENTO, similarity))
        
        # Remover duplicatas e ordenar por confianÃ§a
        alternatives = list(set(alternatives))
        alternatives.sort(key=lambda x: x[1], reverse=True)
        
        return alternatives[:3]  # Top 3 alternativas
    
    def _extract_discourse_markers(self, text: str) -> List[str]:
        """ğŸ’¬ EXTRAIR MARCADORES DE DISCURSO"""
        markers = []
        
        for marker_type, marker_list in self.discourse_analyzers['discourse_markers'].items():
            for marker in marker_list:
                if marker in text.lower():
                    markers.append(f"{marker_type}:{marker}")
        
        return markers
    
    def _calculate_pragmatic_inference(self, intent: ContextualIntent, pragmatic_analysis: Dict, memory: ConversationMemory) -> Dict[str, float]:
        """ğŸ­ CALCULAR INFERÃŠNCIA PRAGMÃTICA"""
        inferences = {}
        
        # InferÃªncias baseadas em implicaturas
        for implicature_name, implicature_data in pragmatic_analysis.get('implicatures', {}).items():
            inferences[implicature_name] = implicature_data['confidence']
        
        # InferÃªncias contextuais
        contextual_infs = pragmatic_analysis.get('contextual_inferences', {})
        inferences.update(contextual_infs)
        
        return inferences
    
    async def _analyze_contextual_coherence(self, intent: ContextualIntent, memory: ConversationMemory) -> float:
        """ğŸ”— ANALISAR COERÃŠNCIA CONTEXTUAL"""
        if not memory.intent_history:
            return 0.8  # Primeira mensagem tem coerÃªncia neutra
        
        # Pegar Ãºltimas 3 intenÃ§Ãµes
        recent_intents = [intent_data[0] for intent_data in memory.intent_history[-3:]]
        current_intent = intent.intent.value
        
        # Calcular coerÃªncia baseada na matriz de similaridade
        coherence_scores = []
        
        for past_intent in recent_intents:
            if past_intent in self.intent_similarity_matrix:
                if current_intent in self.intent_similarity_matrix[past_intent]:
                    coherence_scores.append(self.intent_similarity_matrix[past_intent][current_intent])
                else:
                    coherence_scores.append(0.3)  # Baixa coerÃªncia para intenÃ§Ãµes nÃ£o relacionadas
        
        return sum(coherence_scores) / len(coherence_scores) if coherence_scores else 0.5
    
    async def _calculate_intent_certainty(self, intent: ContextualIntent, linguistic_analysis: Dict) -> float:
        """âœ… CALCULAR CERTEZA DA INTENÃ‡ÃƒO"""
        certainty_factors = []
        
        # Fator 1: ConfianÃ§a base da intenÃ§Ã£o
        certainty_factors.append(intent.confidence)
        
        # Fator 2: Similaridade semÃ¢ntica
        certainty_factors.append(intent.semantic_similarity)
        
        # Fator 3: CoerÃªncia contextual
        certainty_factors.append(intent.contextual_coherence)
        
        # Fator 4: Complexidade linguÃ­stica (menos complexo = mais certo)
        linguistic_certainty = 1.0 - linguistic_analysis.get('overall_complexity', 0.5)
        certainty_factors.append(linguistic_certainty)
        
        # Fator 5: PresenÃ§a de entidades relevantes
        entity_certainty = min(len(intent.entities) * 0.2, 1.0)
        certainty_factors.append(entity_certainty)
        
        # Calcular mÃ©dia ponderada
        weights = [0.3, 0.2, 0.2, 0.15, 0.15]  # Soma = 1.0
        weighted_certainty = sum(factor * weight for factor, weight in zip(certainty_factors, weights))
        
        return min(weighted_certainty, 1.0)
    
    async def _update_ultra_conversation_memory(self, phone: str, intent: ContextualIntent, text: str, linguistic_analysis: Dict):
        """ğŸ§  ATUALIZAR MEMÃ“RIA ULTRA AVANÃ‡ADA"""
        memory = self.conversation_memories[phone]
        
        # Atualizar histÃ³rico de intenÃ§Ãµes
        memory.intent_history.append((
            intent.intent.value, 
            intent.confidence, 
            datetime.now()
        ))
        
        # Atualizar jornada emocional
        memory.emotional_journey.append((
            intent.emotional_state,
            intent.confidence,
            datetime.now()
        ))
        
        # Atualizar padrÃµes de conversaÃ§Ã£o
        memory.conversation_patterns.append(text[:100])  # Primeiros 100 chars
        
        # Detectar mudanÃ§as de contexto
        if len(memory.intent_history) > 1:
            last_intent = memory.intent_history[-2][0]
            if intent.intent.value != last_intent:
                if intent.contextual_coherence < 0.4:  # MudanÃ§a abrupta
                    memory.context_switches.append(datetime.now())
        
        # Atualizar dados de aprendizado
        memory.learning_data['total_messages'] = memory.learning_data.get('total_messages', 0) + 1
        memory.learning_data['avg_confidence'] = (
            memory.learning_data.get('avg_confidence', 0.5) + intent.confidence
        ) / 2
        
        # Manter apenas Ãºltimos 50 registros de cada tipo
        memory.intent_history = memory.intent_history[-50:]
        memory.emotional_journey = memory.emotional_journey[-50:]
        memory.conversation_patterns = memory.conversation_patterns[-50:]
        memory.context_switches = memory.context_switches[-20:]
    
    async def _learn_from_interaction(self, phone: str, intent: ContextualIntent, semantic_analysis: Dict):
        """ğŸ“ APRENDER A PARTIR DA INTERAÃ‡ÃƒO"""
        # Armazenar padrÃµes bem-sucedidos para aprendizado futuro
        if intent.confidence > 0.8:
            pattern_key = f"{intent.intent.value}_{intent.emotional_state}"
            
            if pattern_key not in self.pattern_learning_db:
                self.pattern_learning_db[pattern_key] = []
            
            # Armazenar caracterÃ­sticas da mensagem bem entendida
            learning_pattern = {
                'semantic_clusters': intent.semantic_clusters,
                'entities_count': len(intent.entities),
                'discourse_markers': intent.discourse_markers,
                'confidence': intent.confidence,
                'timestamp': datetime.now()
            }
            
            self.pattern_learning_db[pattern_key].append(learning_pattern)
            
            # Manter apenas Ãºltimos 20 padrÃµes por tipo
            self.pattern_learning_db[pattern_key] = self.pattern_learning_db[pattern_key][-20:]
    
    async def _generate_ultra_contextual_response(
        self, phone: str, intent: ContextualIntent, entities: List[ExtractedEntity], 
        memory: ConversationMemory, semantic_analysis: Dict
    ) -> Dict[str, Any]:
        """ğŸ­ GERAÃ‡ÃƒO ULTRA INTELIGENTE DE RESPOSTA NÃVEL CHATGPT"""
        
        # Usar gerador existente como base
        base_response = await self._generate_contextual_response(phone, intent, entities, {})
        
        # ENRIQUECER COM INTELIGÃŠNCIA ULTRA AVANÃ‡ADA
        
        # 1. PersonalizaÃ§Ã£o baseada em memÃ³ria
        personalization = self._generate_personalized_elements(memory, intent)
        
        # 2. AdaptaÃ§Ã£o baseada em certeza
        certainty_adaptation = self._adapt_response_for_certainty(intent.intent_certainty)
        
        # 3. ContextualizaÃ§Ã£o semÃ¢ntica
        semantic_context = self._add_semantic_context(semantic_analysis, intent)
        
        # 4. Resposta dinÃ¢mica baseada em padrÃµes aprendidos
        learned_enhancements = self._apply_learned_patterns(intent, memory)
        
        # Gerar resposta ultra contextualizada
        ultra_response_text = self._compose_ultra_response(
            base_response['text'], personalization, certainty_adaptation, 
            semantic_context, learned_enhancements, intent
        )
        
        return {
            'text': ultra_response_text,
            'intent': intent.intent.value,
            'confidence': intent.confidence,
            'entities_detected': len(entities),
            'emotional_state': intent.emotional_state,
            'multiple_intents': len(intent.multiple_intents),
            'context_enhanced': True,
            'response_type': 'ultra_mega_contextual',
            
            # NOVOS CAMPOS ULTRA AVANÃ‡ADOS
            'semantic_similarity': intent.semantic_similarity,
            'contextual_coherence': intent.contextual_coherence,
            'intent_certainty': intent.intent_certainty,
            'personalization_level': len(personalization),
            'semantic_clusters': intent.semantic_clusters,
            'discourse_markers': intent.discourse_markers,
            'ultra_enhanced': True
        }
    
    def _generate_personalized_elements(self, memory: ConversationMemory, intent: ContextualIntent) -> Dict[str, str]:
        """ğŸ‘¤ GERAR ELEMENTOS PERSONALIZADOS"""
        personalization = {}
        
        # Baseado em padrÃ£o emocional
        if memory.emotional_journey:
            recent_emotions = [emotion for emotion, _, _ in memory.emotional_journey[-3:]]
            if recent_emotions.count('frustrado') >= 2:
                personalization['empathy'] = "Eu vejo que vocÃª estÃ¡ passando por uma situaÃ§Ã£o chata"
            elif recent_emotions.count('urgente') >= 2:
                personalization['urgency_ack'] = "Entendo que isso Ã© urgente para vocÃª"
        
        # Baseado em histÃ³rico de intenÃ§Ãµes
        if memory.intent_history:
            common_intents = Counter([intent for intent, _, _ in memory.intent_history])
            most_common = common_intents.most_common(1)[0][0]
            if most_common == 'fatura_solicitar':
                personalization['context'] = "Como sempre, vou buscar sua fatura"
        
        return personalization
    
    def _adapt_response_for_certainty(self, certainty: float) -> Dict[str, str]:
        """âœ… ADAPTAR RESPOSTA BASEADA NA CERTEZA"""
        if certainty > 0.9:
            return {'confidence_marker': '**CERTEZA ABSOLUTA!**', 'action': 'Vou resolver isso AGORA!'}
        elif certainty > 0.7:
            return {'confidence_marker': '**ENTENDI PERFEITAMENTE!**', 'action': 'Vou cuidar disso!'}
        elif certainty > 0.5:
            return {'confidence_marker': '**ACHO QUE ENTENDI!**', 'action': 'Deixe-me confirmar...'}
        else:
            return {'confidence_marker': '**HMMMM...**', 'action': 'Me explica melhor?'}
    
    def _add_semantic_context(self, semantic_analysis: Dict, intent: ContextualIntent) -> Dict[str, str]:
        """ğŸ§  ADICIONAR CONTEXTO SEMÃ‚NTICO"""
        context = {}
        
        if semantic_analysis.get('best_match'):
            pattern_id, score = semantic_analysis['best_match']
            if score > 0.8:
                context['semantic_confidence'] = f"Detectei {int(score*100)}% de certeza"
        
        return context
    
    def _apply_learned_patterns(self, intent: ContextualIntent, memory: ConversationMemory) -> Dict[str, str]:
        """ğŸ“ APLICAR PADRÃ•ES APRENDIDOS"""
        enhancements = {}
        
        pattern_key = f"{intent.intent.value}_{intent.emotional_state}"
        if pattern_key in self.pattern_learning_db:
            patterns = self.pattern_learning_db[pattern_key]
            if patterns:
                # Aplicar insights dos padrÃµes aprendidos
                avg_confidence = sum(p['confidence'] for p in patterns) / len(patterns)
                if avg_confidence > 0.8:
                    enhancements['learned_boost'] = "Baseado no que aprendi com vocÃª"
        
        return enhancements
    
    def _compose_ultra_response(
        self, base_text: str, personalization: Dict, certainty: Dict, 
        semantic: Dict, learned: Dict, intent: ContextualIntent
    ) -> str:
        """ğŸ­ COMPOR RESPOSTA ULTRA AVANÃ‡ADA"""
        
        # ComeÃ§ar com texto base
        response_parts = [base_text]
        
        # Adicionar personalizaÃ§Ã£o
        if personalization.get('empathy'):
            response_parts.insert(0, personalization['empathy'] + ".")
        
        # Adicionar marcador de confianÃ§a
        if certainty.get('confidence_marker'):
            response_parts[0] = response_parts[0].replace(
                response_parts[0].split()[0], 
                certainty['confidence_marker']
            )
        
        # Adicionar contexto semÃ¢ntico se alta confianÃ§a
        if semantic.get('semantic_confidence'):
            response_parts.append(f"\n\nğŸ¯ {semantic['semantic_confidence']} no que vocÃª quis dizer!")
        
        # Adicionar insights aprendidos
        if learned.get('learned_boost'):
            response_parts.append(f"\n\nğŸ§  {learned['learned_boost']}, sei exatamente o que fazer!")
        
        return " ".join(response_parts)
    
    async def _ultra_intelligent_fallback(self, phone: str, text: str, error: Exception) -> Dict[str, Any]:
        """ğŸ›¡ï¸ FALLBACK ULTRA INTELIGENTE MULTI-CAMADAS"""
        
        logger.error(f"ğŸš€ Ativando fallback ultra inteligente para: {text[:50]}... | Erro: {error}")
        
        # Tentar fallbacks em cascata
        for fallback_level in self.intelligent_fallback_system['fallback_levels']:
            try:
                if fallback_level['name'] == 'semantic_similarity':
                    return await self._fallback_semantic_similarity(text, fallback_level['threshold'])
                elif fallback_level['name'] == 'keyword_extraction':
                    return await self._fallback_keyword_extraction(text, fallback_level['threshold'])
                elif fallback_level['name'] == 'pattern_matching':
                    return await self._fallback_pattern_matching(text, fallback_level['threshold'])
                elif fallback_level['name'] == 'conversational_context':
                    return await self._fallback_conversational_context(phone, text, fallback_level['threshold'])
                elif fallback_level['name'] == 'intelligent_guess':
                    return await self._fallback_intelligent_guess(text, fallback_level['threshold'])
                    
            except Exception as fallback_error:
                logger.warning(f"Fallback nÃ­vel {fallback_level['level']} falhou: {fallback_error}")
                continue
        
        # Fallback final de emergÃªncia
        return {
            'text': "ğŸ¤” **NOSSA!** Essa foi difÃ­cil atÃ© para mim! Pode tentar falar de um jeito mais simples? Tipo: 'quero minha conta' ou 'quanto devo'?",
            'intent': 'emergency_fallback',
            'confidence': 0.1,
            'fallback_level': 'emergency',
            'ultra_enhanced': True
        }
    
    async def _fallback_semantic_similarity(self, text: str, threshold: float) -> Dict[str, Any]:
        """ğŸ” FALLBACK POR SIMILARIDADE SEMÃ‚NTICA"""
        # Tentar encontrar padrÃ£o semÃ¢ntico mais prÃ³ximo
        best_match = None
        best_score = 0.0
        
        for pattern_id, pattern in self.semantic_patterns.items():
            score = self._calculate_semantic_similarity(text, pattern)
            if score > best_score and score > threshold:
                best_match = pattern_id
                best_score = score
        
        if best_match:
            intent_mapping = {
                'fatura_semantic': 'fatura_solicitar',
                'valor_semantic': 'fatura_valor',
                'tempo_semantic': 'fatura_vencimento',
                'negociacao_semantic': 'negociacao_parcelamento'
            }
            
            inferred_intent = intent_mapping.get(best_match, 'fatura_solicitar')
            
            return {
                'text': f"ğŸ¯ **ENTENDI PELO CONTEXTO!** VocÃª quer algo relacionado a {inferred_intent.replace('_', ' ')}. Vou ajudar!",
                'intent': inferred_intent,
                'confidence': best_score,
                'fallback_level': 'semantic_similarity',
                'ultra_enhanced': True
            }
        
        raise Exception("Similaridade semÃ¢ntica insuficiente")
    
    async def _fallback_keyword_extraction(self, text: str, threshold: float) -> Dict[str, Any]:
        """ğŸ”‘ FALLBACK POR EXTRAÃ‡ÃƒO DE PALAVRAS-CHAVE"""
        keywords = {
            'fatura': ['conta', 'boleto', 'fatura', 'segunda', 'via', 'papel'],
            'valor': ['quanto', 'valor', 'devo', 'pagar', 'preÃ§o', 'dinheiro'],
            'vencimento': ['quando', 'vence', 'prazo', 'data', 'atÃ©'],
            'negociacao': ['parcelar', 'acordo', 'desconto', 'negociar', 'facilitar']
        }
        
        scores = {}
        for intent, intent_keywords in keywords.items():
            score = sum(1 for keyword in intent_keywords if keyword in text.lower())
            if score > 0:
                scores[intent] = score / len(intent_keywords)
        
        if scores:
            best_intent = max(scores.items(), key=lambda x: x[1])
            if best_intent[1] > threshold:
                return {
                    'text': f"ğŸ” **CAPTEI!** Pelas palavras-chave, vocÃª quer {best_intent[0]}. Ã‰ isso mesmo?",
                    'intent': best_intent[0],
                    'confidence': best_intent[1],
                    'fallback_level': 'keyword_extraction',
                    'ultra_enhanced': True
                }
        
        raise Exception("Palavras-chave insuficientes")
    
    async def _fallback_pattern_matching(self, text: str, threshold: float) -> Dict[str, Any]:
        """ğŸ§© FALLBACK POR CORRESPONDÃŠNCIA DE PADRÃ•ES"""
        # PadrÃµes de emergÃªncia muito bÃ¡sicos
        emergency_patterns = [
            (r'\b(conta|boleto|fatura)\b', 'fatura_solicitar', 0.7),
            (r'\b(quanto|valor)\b', 'fatura_valor', 0.6),
            (r'\b(quando|vence|prazo)\b', 'fatura_vencimento', 0.6),
            (r'\b(paguei|pago)\b', 'pagamento_confirmacao', 0.5),
            (r'\b(parcelar|acordo)\b', 'negociacao_parcelamento', 0.5),
        ]
        
        for pattern, intent, confidence in emergency_patterns:
            if re.search(pattern, text, re.IGNORECASE) and confidence > threshold:
                return {
                    'text': f"ğŸ§© **CONSEGUI ENTENDER!** Pelo padrÃ£o, vocÃª quer {intent.replace('_', ' ')}!",
                    'intent': intent,
                    'confidence': confidence,
                    'fallback_level': 'pattern_matching',
                    'ultra_enhanced': True
                }
        
        raise Exception("Nenhum padrÃ£o corresponde")
    
    async def _fallback_conversational_context(self, phone: str, text: str, threshold: float) -> Dict[str, Any]:
        """ğŸ’­ FALLBACK POR CONTEXTO CONVERSACIONAL"""
        if phone in self.conversation_memories:
            memory = self.conversation_memories[phone]
            if memory.intent_history:
                # Assumir que Ã© follow-up da Ãºltima intenÃ§Ã£o
                last_intent, last_confidence, _ = memory.intent_history[-1]
                
                if last_confidence > threshold:
                    return {
                        'text': f"ğŸ’­ **PELO CONTEXTO!** VocÃª ainda estÃ¡ falando sobre {last_intent.replace('_', ' ')}, nÃ©?",
                        'intent': last_intent,
                        'confidence': last_confidence * 0.8,
                        'fallback_level': 'conversational_context',
                        'ultra_enhanced': True
                    }
        
        raise Exception("Contexto conversacional insuficiente")
    
    async def _fallback_intelligent_guess(self, text: str, threshold: float) -> Dict[str, Any]:
        """ğŸ² FALLBACK POR SUPOSIÃ‡ÃƒO INTELIGENTE"""
        # Se chegou atÃ© aqui, fazer uma suposiÃ§Ã£o educada baseada no contexto de cobranÃ§a
        text_length = len(text.split())
        
        if text_length <= 3:
            # Texto muito curto - provavelmente quer fatura
            guess_intent = 'fatura_solicitar'
            guess_confidence = 0.4
        elif '?' in text:
            # Tem pergunta - provavelmente quer informaÃ§Ã£o (valor ou vencimento)
            guess_intent = 'fatura_valor'
            guess_confidence = 0.3
        else:
            # Default para solicitaÃ§Ã£o de fatura
            guess_intent = 'fatura_solicitar'
            guess_confidence = 0.2
        
        if guess_confidence > threshold:
            return {
                'text': f"ğŸ² **VAMOS TENTAR!** Pelo contexto geral, acho que vocÃª quer {guess_intent.replace('_', ' ')}. Se nÃ£o for isso, me fala 'nÃ£o' que eu entendo outra coisa!",
                'intent': guess_intent,
                'confidence': guess_confidence,
                'fallback_level': 'intelligent_guess',
                'ultra_enhanced': True,
                'requires_confirmation': True
            }
        
        raise Exception("ImpossÃ­vel fazer suposiÃ§Ã£o vÃ¡lida") 